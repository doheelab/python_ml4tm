{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version check\n",
    "\n",
    "우리는 scikit learn 에서 제공하는 20NewsGroups data 를 이용하여 document classification 을 하는 multi-layer feed-foward neural network 를 만들어봅니다.\n",
    "\n",
    "현재 실습의 torch 버전은 1.0.1 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "20NewsGroups 의 모든 데이터를 이용해도 되지만, 빠른 확인을 위하여 네 개의 카테고리만 이용합니다. 20NewsGroups 은 20 개의 카테고리로 분류된 뉴스 문서 집합입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set and test set\n",
    "categories = [\n",
    "    'rec.sport.baseball',\n",
    "    'soc.religion.christian',\n",
    "    'comp.windows.x',\n",
    "    'sci.space'\n",
    "]\n",
    "\n",
    "removals = ('headers', 'footers', 'quotes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove 에 'headers', 'footers', 'quotes' 를 넣으면 뉴스의 header 들이 제거 된 text 만 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train', remove=removals, categories=categories)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test', remove=removals, categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 text 와 category label 을 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = newsgroups_train.data\n",
    "y_train = newsgroups_train.target\n",
    "data_test = newsgroups_test.data\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "\n",
    "TF-IDF vectorizer 를 이용하여 vectorizing 을 합니다. 이 때 학습 데이터 기준에서 word index 가 학습되도록 학습데이터에는 fit_transform 을 적용하고, 테스트 데이터에는 transform 을 적용합니다. 학습 데이터에 존재하지 않는 단어는 테스트 단어에서 벡터화 되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x = (2382, 6535), y = (2382,)\n",
      "test  x = (1584, 6535), y = (1584,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "x_train = vectorizer.fit_transform(data_train)\n",
    "x_test = vectorizer.transform(data_test)\n",
    "\n",
    "print('train x = {}, y = {}'.format(x_train.shape, y_train.shape))\n",
    "print('test  x = {}, y = {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader\n",
    "\n",
    "DataSet 은 DataLoader 의 input 입니다. 각 index 에 해당하는 값을 return 하도록 getitem 과 데이터셋의 크기를 측정할 수 있는 len 만 구현하면 됩니다. 만약 데이터가 너무 큰 경우에는 init 에 파일 주소를 입력받고, getitem 에서 파일을 하나씩 읽어도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len trainset = 2382\n",
      "len testset = 1584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor([0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NewsGroupDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = torch.FloatTensor(self.X[i].todense())[0]\n",
    "        y = torch.LongTensor(self.y[i:i+1])\n",
    "        return x, y\n",
    "\n",
    "train_set = NewsGroupDataset(x_train, y_train)\n",
    "test_set = NewsGroupDataset(x_test, y_test)\n",
    "\n",
    "print('len trainset = {}'.format(len(train_set)))\n",
    "print('len testset = {}'.format(len(test_set)))\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader 는 Dataset 을 이용하여 minibatch 를 만들어주고 shuffling 도 지원해주는 유틸입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6535])\n",
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in train_loader:\n",
    "    print(x_batch.size())\n",
    "    print(y_batch.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습데이터는 2382 개였지만, batch size 가 16 이기 때문에 mini batch 의 개수는 149 개 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Feed forward neural netowrk 를 만듭니다. torch.nn.Module 을 상속하여 init, forward 함수만 구현하면 됩니다.\n",
    "\n",
    "init 을 구현할 때에는 Python 의 상속처럼 super().\\_\\_init\\_\\_() 을 실행해야 합니다. 우리는 3 개의 hidden layer 로 이뤄진 feed forward 를 만들 것입니다. 각각은 (input, hidden 1), (hidden 1, hidden 2), (hidden 2, classes) 의 크기로 이뤄진 weight matrix 를 지닙니다. bias 역시 학습하도록 설정합니다.\n",
    "\n",
    "```python\n",
    "self.fc_1 = nn.Linear(in_features = input_dim, out_features = hidden_1_dim, bias=True)\n",
    "self.fc_2 = nn.Linear(in_features = hidden_1_dim, out_features = hidden_2_dim, bias=True)\n",
    "self.fc_3 = nn.Linear(in_features = hidden_2_dim, out_features = n_classes, bias = True)\n",
    "```\n",
    "\n",
    "우리는 Linear layer 만을 만들었을 뿐, activation function 은 아직 만들지 않았습니다. hidden layer 1, 2 의 output 에 대하여 ReLU 를 적용합니다.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    out = F.relu(self.fc_1(x))\n",
    "    out = F.relu(self.fc_2(out))\n",
    "    return self.fc_3(out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__ (self, input_dim, hidden_1_dim, hidden_2_dim, n_classes):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_1_dim, bias=True)\n",
    "        self.fc_2 = nn.Linear(hidden_1_dim, hidden_2_dim, bias=True)\n",
    "        self.fc_3 = nn.Linear(hidden_2_dim, n_classes, bias = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc_1(x))\n",
    "        out = F.relu(self.fc_2(out))\n",
    "        return self.fc_3(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "numpy.unique 는 numpy.ndarray 의 값의 set 입니다. 네 개의 카테고리가 각각 0, 1, 2, 3 으로 encoding 되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 이용할 패러매터를 정의합니다. epochs, mini batch size, hidden 1, hidden 2 의 크기를 정의합니다.\n",
    "\n",
    "우리가 정의한 네트워크의 구조는 아래와 같습니다. 6,535 개의 단어로 표현된 문서가 128, 32 차원을 거쳐 4 개의 클래스로 분류됩니다.\n",
    "\n",
    "    6535 - 128 - 32 - 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer = 6535 - 128 - 32 - 4\n",
      "n data = 2382\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 10\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "hidden_1_dim = 128\n",
    "hidden_2_dim = 32\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "n_data = x_train.shape[0]\n",
    "\n",
    "print('layer = {} - {} - {} - {}'.format(\n",
    "    input_dim, hidden_1_dim, hidden_2_dim, n_classes))\n",
    "print('n data = {}'.format(n_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, loss function, optimizer\n",
    "\n",
    "실제로 모델을 만듭니다. loss function (criterion) 과 optimizer 는 regression 과 비슷합니다. 단, classification 이기 때문에 Cross Entropy loss 를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = FeedForwardNN(\n",
    "    input_dim,\n",
    "    hidden_1_dim,\n",
    "    hidden_2_dim,\n",
    "    n_classes\n",
    ")\n",
    "\n",
    "# Parameter for the optimizer\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의한 바와 같이 6535 - 128 - 32 구조의 hidden layer 를 지닌, 4 개의 클래스를 분류하는 neural network 가 만들어졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNN(\n",
      "  (fc_1): Linear(in_features=6535, out_features=128, bias=True)\n",
      "  (fc_2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc_3): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function\n",
    "\n",
    "epochs 만큼 iteration 을 돕니다. 각 epoch 마다의 누적 loss 를 저장하기 위한 준비를 합니다.\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    # TODO\n",
    "```\n",
    "\n",
    "항상 optimizer 에 이전 step 에서 이용한 gradient 를 지운 뒤, forwarding, back-propagation 을 합니다.\n",
    "\n",
    "```python\n",
    "for i in range(n_data // batch_size):\n",
    "    # ...\n",
    "    optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "prediction, loss 계산, 그리고 back propagation 을 수행합니다.\n",
    "\n",
    "```python\n",
    "for i in range(n_data // batch_size):\n",
    "    # ...\n",
    "    y_pred = model(x_batch)\n",
    "    loss = criterion(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "loss 의 data 에는 loss 값이 저장되어 있습니다. 이는 torch.Tensor 이니 numpy() 를 이용하여 숫자로 변환하여 loss_sum 에 누적합니다. 매 epoch 마다 loss 가 줄어듦을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0/10 training loss = 0.9946g loss = 0.9946\n",
      "epoch = 1/10 training loss = 0.1481g loss = 0.1481\n",
      "epoch = 2/10 training loss = 0.05485 loss = 0.05485\n",
      "epoch = 3/10 training loss = 0.03889 loss = 0.03889\n",
      "epoch = 4/10 training loss = 0.03313 loss = 0.03313\n",
      "epoch = 5/10 training loss = 0.03105 loss = 0.03105\n",
      "epoch = 6/10 training loss = 0.02961 loss = 0.02961\n",
      "epoch = 7/10 training loss = 0.03051 loss = 0.03051\n",
      "epoch = 8/10 training loss = 0.0299g loss = 0.02992\n",
      "epoch = 9/10 training loss = 0.02974 loss = 0.02974\n"
     ]
    }
   ],
   "source": [
    "def train(loader, model, loss_func, optimizer, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        n_batches = len(loader)\n",
    "\n",
    "        for i, (x_batch, y_batch) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            # size: (batch, 1) -> (batch,)\n",
    "            y_batch = y_batch.squeeze(dim=1)\n",
    "            loss = loss_func(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.data.numpy()\n",
    "            if i % 10 == 0:\n",
    "                loss_tmp = loss_sum / (i+1)\n",
    "                print(f'\\repoch = {epoch}, batch = {i+1}/{n_batches}, training loss = {loss_tmp:.4}', end='')\n",
    "        print(f'\\repoch = {epoch}/{epochs} training loss = {loss_tmp:.4}')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train(train_loader, model, loss_func, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy\n",
    "\n",
    "테스트 성능도 DataLoader 를 이용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch, y_batch in test_loader:\n",
    "    y_batch = y_batch.squeeze(dim=1)\n",
    "    y_pred = model(x_batch)\n",
    "    score, indices = torch.max(y_pred.data, dim=1)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred 의 size 는 (batch, class) 입니다. torch.max 는 torch.Tensor 에 대하여 dim 기준으로 최대값과 그 값을 지니는 index 를 출력합니다. (batch, class) size 에서 class column 기준으로 max 값을 찾습니다. return 은 max value 와 index 로 나뉘어져 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 4])\n",
      "torch.Size([64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(y_batch.size())\n",
    "print(y_pred.size())\n",
    "print(score.size(), indices.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0251, 11.4719,  9.8125])\n",
      "tensor([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(score[:3])\n",
    "print(indices[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 dropout 과 같은 기법은 학습 시에는 작동하고, 테스트 시에는 작동하지 않습니다. 그렇기 때문에 테스트 시에는 nn.Module 에 eval() 함수를, 학습 시에는  train() 함수를 실행하여 각각의 모드가 실행될 수 있도록 합니다. eval() 이 한 번 실행되면 모델은 더 학습되지 않습니다. 추가 학습을 하려면 반드시 train() 을 실행시켜야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy = 0.911\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(model, test_loader):\n",
    "    n_corrects, n_total = 0, 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        y_batch = y_batch.squeeze(dim=1)\n",
    "        y_pred = model(x_batch)\n",
    "        score, indices = torch.max(y_pred.data, dim=1)\n",
    "        n_corrects += (indices == y_batch).sum().numpy()\n",
    "        n_total += y_pred.size()[0]\n",
    "    accuracy = n_corrects / n_total\n",
    "    return accuracy\n",
    "\n",
    "model.eval()\n",
    "accuracy = test_accuracy(model, test_loader)\n",
    "print(f'test accuracy = {accuracy:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO\n",
    "\n",
    "학습된 모델은 save 를 이용하여 저장하고 load 를 이용하여 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FeedForwardNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model_path = 'newsgroup_net.pt'\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9109848484848485"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = torch.load(model_path)\n",
    "loaded_model.eval()\n",
    "test_accuracy(loaded_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
