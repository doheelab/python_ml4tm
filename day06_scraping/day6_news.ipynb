{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.7.x 에서 뉴스의 본문일부가 파싱이 제대로 되지 않는 문제가 있습니다. 이는 Python 3.6.x 에서 작성하였습니다.\n",
    "\n",
    "이전의 네이버 영화 스크랩 코드에서 작성한 get_soup 과 normalize 함수를 utils.py 에 넣어뒀습니다. 이를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version = sys.version_info(major=3, minor=6, micro=2, releaselevel='final', serial=0)\n",
      "BeautifulSoup version = 4.7.1\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from utils import get_soup\n",
    "from utils import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 링크의 뉴스 기사를 가져와 제목과 본문을 가져옵니다. 이는 앞서 연습한 것처럼 inspect 를 이용하여 HTML 태그를 살펴보면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=102&oid=422&aid=0000370702'\n",
    "soup = get_soup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"강남경찰서 또 비위…피의자에게 금품수수 '입건'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_title(soup):\n",
    "    return normalize(soup.select('h3[id=articleTitle]')[0].text)\n",
    "\n",
    "parse_title(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "때로 br 태그를 이용하여 줄바꿈을 하는 경우들이 있는데, 이 정보를 살리기 위해 br 태그를 str 로 변환하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_content(soup):\n",
    "    html = str(soup.select(\"div[id=articleBodyContents]\")[0])\n",
    "    html = html.replace('<br/>', 'br/')\n",
    "    content = BeautifulSoup(html, 'lxml').text\n",
    "    content = '  '.join(normalize(s) for s in content.split('br/') if s.strip())\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"동영상 뉴스 // flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {} 강남경찰서 또 비위…피의자에게 금품수수 '입건'\",\n",
       " '[앵커]',\n",
       " '서울 강남경찰서 소속의 한 경찰관이 피의자에게 금품과 접대를 받은 사실이 드러났습니다.',\n",
       " '버닝썬 사태와 관련해 강남경찰서 소속 전·현직 경찰관의 유착정황이 드러난 데 이어 또 비슷한 일이 발생한 겁니다.',\n",
       " '조한대 기자입니다.',\n",
       " '[기자]',\n",
       " '경찰이 서울 강남경찰서 경제팀 소속 A경위를 금품수수 혐의로 입건했습니다.',\n",
       " '금품을 준 인물은 A경위가 2017년에 맡았던 사기 사건의 여성 피의자.',\n",
       " \"당시 이 사건은 '혐의 없음' 의견으로 검찰에 넘겨졌습니다.\",\n",
       " '경찰은 사건 송치 후 두 사람이 친분을 쌓은 사실을 확인했습니다.',\n",
       " '서울경찰청 지능범죄수사대는 \"현재 골프접대 1회, 금품 수백만원을 받은 것을 확인했다\"며 \"16일 A경위의 주거지와 사무실 등을 압수수색했다\"고 밝혔습니다.',\n",
       " '강남경찰서를 거친 경찰관의 비위 정황은 이뿐만이 아닙니다.',\n",
       " '앞서 전 강남경찰서 과장인 석모 경정은 클럽 버닝썬과 경찰 간 유착 고리로 지목된 전직 경찰 강모씨에게 수입 중고차를 싸게 사들여 입건됐습니다.',\n",
       " \"승리 등이 있던 대화방에서 '경찰총장'으로 거론된 윤모 총경은 강남경찰서에서 생활안전과장을 지냈습니다.\",\n",
       " \"윤 총경은 술집 '몽키뮤지엄'이 식품위생법 위반으로 고발 됐을때, 강남경찰서 팀장급 직원 등에게 전화해 수사 상황을 알아본 것으로 알려졌습니다.\",\n",
       " '클럽과의 유착 의혹으로 얽힌 강남경찰서.',\n",
       " \"소속 직원의 비위 정황이 끊임없이 드러나면서 '비리 경찰서'라는 오명을 쓰게될 위기에 처했습니다.\",\n",
       " '연합뉴스TV 조한대입니다.',\n",
       " 'onepunch@yna.co.kr',\n",
       " '연합뉴스TV 기사문의 및 제보 : 카톡/라인 jebo23',\n",
       " '▶ 연합뉴스TV 네이버 채널 구독 ▶ 생방송 시청',\n",
       " '▶ 대한민국 뉴스의 시작, 연합뉴스TV 앱 다운받기']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_content(soup).split('  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드를 정리하면 scrap_news 라는 함수를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_news(url):\n",
    "    json_obj = {'url': url}\n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "        json_obj['title'] = parse_title(soup)\n",
    "        json_obj['content'] = parse_content(soup)\n",
    "        return json_obj\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# scrap_news(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색어를 입력하여 뉴스 기사들의 urls 을 가져오기 위한 함수도 만들어 봅니다. URL 에는 한글이 입력될 수 없습니다. 이들은 다른 글자로 인코딩이 되어야 합니다. URL 에 한글이 입력되어 있다면 이를 복사하여 다른 메모장에 붙여보시면 16진법으로 기술된 글자가 나옵니다. 한글이 16진법 숫자로 변환된 것인데, urllib 에서 이러한 기능을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def get_search_url(query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    query : str\n",
    "        질의어\n",
    "    start_date : str\n",
    "        yyyy.mm.dd 형식 ex) 2019.04.18\n",
    "    end_date : str\n",
    "        yyyy.mm.dd 형식 ex) 2019.04.18\n",
    "    \"\"\"\n",
    "    base = 'https://search.naver.com/search.naver?where=news&query={0}&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={2}'\n",
    "    url = base.format(url_encode(query), start_date, end_date)\n",
    "    return url\n",
    "\n",
    "def url_encode(query, encoding='utf-8'):\n",
    "    def encode_a_term(term):\n",
    "        try:\n",
    "            return urllib.parse.quote(term, encoding=encoding)\n",
    "        except Exception as e:\n",
    "            raise ValueError('Failed to encode query %s' % str(e))\n",
    "    return '+'.join([encode_a_term(term) for term in query.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%EB%B2%84%EB%8B%9D%EC%8D%AC'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_encode('버닝썬')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_encode 함수를 이용하여 검색어, 시작일, 종료일이 입력되었을 때 뉴스 검색 결과를 가져오는 url 을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://search.naver.com/search.naver?where=news&query=%EB%B2%84%EB%8B%9D%EC%8D%AC&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2019.04.18&de=2019.04.19'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = get_search_url('버닝썬', '2019.04.18', '2019.04.19')\n",
    "base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스가 몇 건 검색되었는지 숫자를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_article_num(url):\n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "        if not soup:\n",
    "            return 0\n",
    "        header_text = soup.select('div[class=section_head] div[class^=title_desc] span')\n",
    "        if not header_text:\n",
    "            return 0\n",
    "        header_text = header_text[0].text\n",
    "        header_text = re.findall('[,\\\\d]+건', header_text)[0]\n",
    "        header_text = re.sub(',', '', header_text) # Remove Comma\n",
    "        num_articles = int(header_text[:-1])\n",
    "        return num_articles\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError('Failed to get total number of articles %s' % str(e))\n",
    "\n",
    "get_article_num(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 패턴이 있는 링크만 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_urls(base_url, page):\n",
    "\n",
    "    url_patterns = ('a[href^=\"https://news.naver.com/main/read.nhn?\"]',\n",
    "            'a[href^=\"https://entertain.naver.com/main/read.nhn?\"]',\n",
    "            'a[href^=\"https://sports.news.naver.com/sports/index.nhn?\"]',\n",
    "            'a[href^=\"https://news.naver.com/sports/index.nhn?\"]')\n",
    "\n",
    "    urls_in_page = set()\n",
    "    page_url = '{}&start={}&refresh_start=0'.format(base_url, 1 + 10*(page-1))\n",
    "    soup = get_soup(page_url)\n",
    "    if not soup:\n",
    "        return urls_in_page\n",
    "    try:\n",
    "        article_blocks = soup.select('ul[class=type01]')[0]\n",
    "        for pattern in url_patterns:\n",
    "            article_urls = [link['href'] for link in article_blocks.select(pattern)]\n",
    "            urls_in_page.update(article_urls)\n",
    "    except Exception as e:\n",
    "        raise ValueError('Failed to extract urls from page %s' % str(e))\n",
    "\n",
    "    return urls_in_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색 결과로부터 뉴스 기사의 링크를 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting urls from page = 1\n",
      "getting urls from page = 2\n",
      "getting urls from page = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "num_article = get_article_num(url)\n",
    "num_article = 30 # debug\n",
    "base_url = get_search_url('버닝썬', '2019.04.18', '2019.04.19')\n",
    "\n",
    "urls = []\n",
    "for page in range(1, math.ceil(num_article / 10) + 1):\n",
    "    urls += get_article_urls(base_url, page)    \n",
    "    print('getting urls from page = {}'.format(page))\n",
    "    time.sleep(1)\n",
    "\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "댓글의 경우에는 크롬의 Network 를 이용하여 request url 을 가져올 수 있습니다. 여기에는 header 에 입력되는 user-agent 와 referer 의 값도 함께 기록되어 있습니다. 뉴스 기사의 url 이 referer 입니다. 이를 이용하여 API 주소를 가져옵니다.\n",
    "\n",
    "![](news_comments.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "referer_url = 'https://news.naver.com/main/read.nhn?m_view=1&mode=LSD&mid=shm&sid1=100&oid=008&aid=0004206900'\n",
    "request_url = 'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?ticket=news&templateId=default_politics&pool=cbox5&_callback=jQuery112406445509904006934_1555720412273&lang=ko&country=KR&objectId=news008%2C0004206900&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=2&refresh=false&sort=NEW&current=1700689291&prev=1700692531&includeAllStatus=true&_=1555720412275'\n",
    "headers = {'Referer': referer_url}\n",
    "\n",
    "r = requests.get(request_url, headers=headers)\n",
    "text = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response 의 시작과 끝에는 java script 관련 단어들이 포함되어 있습니다. 이를 제거한 뒤, JSON parsing 을 합니다. json.load 는 파일을 JSON 형식으로 읽는 함수이며, json.loads 는 str 을 JSON 형식으로 읽는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "text = text[text.index('(')+1:-2]\n",
    "response = json.loads(text)\n",
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 정리하면 한 뉴스기사의 댓글을 가져오는 get_response 함수를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_response(request_url, referer_url):\n",
    "    r = requests.get(request_url, headers=headers)\n",
    "    text = r.text\n",
    "    text = text[text.index('(')+1:-2]\n",
    "    return json.loads(text)\n",
    "\n",
    "comments = get_response(request_url, referer_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['success', 'code', 'message', 'lang', 'country', 'result', 'date'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments['result']['commentList'][0]['contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
