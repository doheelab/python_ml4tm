{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 Gensim 을 이용한 FastText 를 학습했습니다. Facebook Research 에서도 FastText 코드를 제공합니다. 설치는 pip install 로 가능합니다. 여기에서는 subword enriching 인 unsupervised word embedding 과 document classification 을 위한 supervised word embedding 두 가지 버전을 모두 제공합니다. 현재 버전은 0.9.1 입니다. 0.8.x 에서 0.9.x 로 버전이 변화하면서 인터페이스가 많이 바뀌었습니다. 이 튜토리얼은 0.9.1 기준으로 작성되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised embedding\n",
    "\n",
    "Unsupervised FastText 는 띄어쓰기 기준으로 단어가 구분되는 데이터셋이면 됩니다. 한글의 경우 초/중/종성을 구분하면 더 좋은 성능을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅋㅡ-ㄹㅣ-ㅅㅡ-ㅌㅗ-ㅍㅓ- ㄴㅗㄹㄹㅏㄴ ㅇㅔ-ㄱㅔ- ㅇㅜ-ㄹㅣ-ㄴㅡㄴ ㄴㅗㄹㄹㅏㄴ ㄷㅏ-\n",
      "ㅇㅣㄴㅅㅔㅂㅅㅕㄴ ㅈㅓㅇㅁㅏㄹ ㅎㅡㅇㅁㅣ-ㅈㅣㄴㅈㅣㄴㅎㅏ-ㄱㅔ- ㅂㅘㅆㅇㅓㅆㄱㅗ- ㅋㅡ-ㄹ\n",
      "ㄴㅗㄹㄹㅏㄴㅇㅣ-ㅁㅕㄴ ㅁㅜ-ㅈㅗ-ㄱㅓㄴ ㅂㅘ-ㅇㅑ- ㄷㅚㄴㄷㅏ- ㅇㅙ-ㄴㅑ-ㅎㅏ-ㅁㅕㄴ \n"
     ]
    }
   ],
   "source": [
    "from lovit_textmining_dataset.navermovie_comments import get_facebook_fasttext_data\n",
    "\n",
    "corpus_path = get_facebook_fasttext_data(large=False, supervise=False)\n",
    "\n",
    "with open(corpus_path, encoding='utf-8') as f:\n",
    "    for _ in range(3):\n",
    "        print(next(f).strip()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_model 을 할 때에는 fasttext_model_name 뒤에 확장자 '.bin' 을 붙여줘야 합니다. .bin 과 .vec 두 가지의 파일을 만들기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_modelname = './fasttext_subword'\n",
    "\n",
    "model = fasttext.train_unsupervised(corpus_path, model='skipgram', minn=3, maxn=6, thread=8)\n",
    "model.save_model(f\"{unsupervised_modelname}.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초/중/종성을 나눴기 때문에 cosine similarity 를 계산할 때에도 입력될 단어를 초/중/종성으로 나눠야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7917058"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from soynlp.hangle import decompose, compose\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def remove_doublespace(s):\n",
    "    doublespace_pattern = re.compile('\\s+')\n",
    "    return doublespace_pattern.sub(' ', s).strip()\n",
    "\n",
    "def encode(s):\n",
    "    def process(c):\n",
    "        if c == ' ':\n",
    "            return c\n",
    "        jamo = decompose(c)\n",
    "        # 'a' or 모음 or 자음\n",
    "        if (jamo is None) or (jamo[0] == ' ') or (jamo[1] == ' '):\n",
    "            return ' '\n",
    "        base = jamo[0]+jamo[1]\n",
    "        if jamo[2] == ' ':\n",
    "            return base + '-'\n",
    "        return base + jamo[2]\n",
    "\n",
    "    s = ''.join(process(c) for c in s)\n",
    "    return remove_doublespace(s).strip()\n",
    "\n",
    "def decode(s):\n",
    "    def process(t):\n",
    "        assert len(t) % 3 == 0\n",
    "        t_ = t.replace('-', ' ')\n",
    "        chars = [tuple(t_[3*i:3*(i+1)]) for i in range(len(t_)//3)]\n",
    "        recovered = [compose(*char) for char in chars]\n",
    "        recovered = ''.join(recovered)\n",
    "        return recovered\n",
    "\n",
    "    return ' '.join(process(t) for t in s.split())\n",
    "\n",
    "def cosine_similarity(word1, word2):\n",
    "    word1 = encode(word1)\n",
    "    word2 = encode(word2)\n",
    "    v1 = model.get_word_vector(word1)\n",
    "    v2 = model.get_word_vector(word2)\n",
    "    cos_sim = dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    return cos_sim\n",
    "\n",
    "cosine_similarity('재미썼어', '재밌었어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅈㅐ-ㅁㅣㅆㅇㅓㅆㅇㅓ-', '<ㅈㅐ', '<ㅈㅐ-']\n",
      "[   8295  446956 1433507]\n"
     ]
    }
   ],
   "source": [
    "subwords, indices = model.get_subwords(encode('재밌었어'))\n",
    "print(subwords[:3])\n",
    "print(indices[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised embedding\n",
    "\n",
    "평점 데이터를 이용하여 classifier 를 학습하는 코드입니다. 8점 이상을 positive, 3점 이하를 negative 라 하였습니다. Supervised FastText 는 앞에 label_prefix 를 입력한 형태의 데이터를 가정합니다. 띄어쓰기 기준으로 prefix 가 붙은 단어는 document label 로 이용합니다. 이 형태로 데이터를 미리 정리해두었습니다. 평점 기준 1 ~ 3 점은 neg (negative), 8 ~ 10 점은 pos (positive) 영화 평으로 생각합니다. 클래스는 반드시 두 개가 아니어도 괜찮습니다. Supervised FastText 는 Softmax 를 이용하기 때문에 multi class classification 을 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__pos ㅋㅡ-ㄹㅣ-ㅅㅡ-ㅌㅗ-ㅍㅓ- ㄴㅗㄹㄹㅏㄴ ㅇㅔ-ㄱㅔ- ㅇㅜ-ㄹㅣ-ㄴ\n",
      "__label__pos ㅇㅣㄴㅅㅔㅂㅅㅕㄴ ㅈㅓㅇㅁㅏㄹ ㅎㅡㅇㅁㅣ-ㅈㅣㄴㅈㅣㄴㅎㅏ-ㄱㅔ- ㅂ\n",
      "__label__pos ㄴㅗㄹㄹㅏㄴㅇㅣ-ㅁㅕㄴ ㅁㅜ-ㅈㅗ-ㄱㅓㄴ ㅂㅘ-ㅇㅑ- ㄷㅚㄴㄷㅏ- \n"
     ]
    }
   ],
   "source": [
    "corpus_path = get_facebook_fasttext_data(large=False, supervise=True)\n",
    "\n",
    "with open(corpus_path, encoding='utf-8') as f:\n",
    "    for _ in range(3):\n",
    "        print(next(f).strip()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_modelname = './fasttext_supervised'\n",
    "\n",
    "model_superv = fasttext.train_supervised(corpus_path, label_prefix='__label__', thread=8)\n",
    "model_superv.save_model(f'{supervised_modelname}.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델의 labels 를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__pos', '__label__neg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_superv.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습데이터의 텍스트가 초/중/종성이 분리되어 있기 때문에 입력되는 문장도 동일한 전처리를 거쳐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅇㅓㄴㅍㅡㄹㅉㅓㄴㄷㅏ- ㅈㅐ-ㅁㅣ-ㅇㅓㅄㄷㅏ- ㅇㅣ-ㅅㅏㅇㅎㅐ-'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '언플쩐다 재미없다 이상해'\n",
    "\n",
    "encode(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier.predict() 는 array of str을 입력받아야 합니다. k는 가장 가까운 클래스 k개의 개수입니다. \n",
    "\n",
    "classifier 는 각 단어에 대하여 각각 class prediction 을 합니다. 이 결과를 통하여 한 문장, 즉 words 의 classification 까지 하는 함수는 제공되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['__label__neg'], ['__label__neg'], ['__label__neg']]\n",
      "[[0.54100609]\n",
      " [0.99834436]\n",
      " [0.99190998]]\n"
     ]
    }
   ],
   "source": [
    "words = encode(sent).split()\n",
    "labels, probs = model_superv.predict(words,k=1)\n",
    "print(labels)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier.predict_prob()는 각 단어에 대하여 가까운 k 개 클래스의 확률을 계산합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 sentiment score 는 각 단어의 score 의 가중합 혹은 평균으로 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와진짜: (pos, 0.9937)\n",
      "개쩐다: (pos, 0.9931)\n",
      "영화: (pos, 0.5784)\n",
      "졸라: (neg, 0.6893)\n",
      "재밌어: (pos, 0.9776)\n",
      "언플: (neg, 1.0)\n",
      "쩌네: (neg, 0.8377)\n"
     ]
    }
   ],
   "source": [
    "words = encode('와진짜 개쩐다 영화 졸라 재밌어 언플 쩌네').split()\n",
    "labels, probs = model_superv.predict(words,k=1)\n",
    "for word, label, prob in zip(words, labels, probs):\n",
    "    word = decode(word)\n",
    "    label = label[0][9:]\n",
    "    prob = float(prob)\n",
    "    print(f'{word}: ({label}, {prob:.4})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어절 단위의 의미도 학습됩니다. `영화라고` 라는 어절은 거의 부정적인 문맥에서 등장했습니다. `씹노잼`과 같은 단어는 당연히 부정적인 의미이고요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "씹노잼: (neg, 0.9241)\n",
      "이걸: (neg, 0.9956)\n",
      "영화라고: (neg, 0.9424)\n",
      "만드냐: (neg, 0.9986)\n"
     ]
    }
   ],
   "source": [
    "words = encode('씹노잼 이걸 영화라고 만드냐').split()\n",
    "labels, probs = model_superv.predict(words,k=1)\n",
    "for word, label, prob in zip(words, labels, probs):\n",
    "    word = decode(word)\n",
    "    label = label[0][9:]\n",
    "    prob = float(prob)\n",
    "    print(f'{word}: ({label}, {prob:.4})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 `영화`라는 subword 를 포함하는 `영화지` 는 긍정적인 문맥에서 등장하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이게: (neg, 0.9499)\n",
      "영화지: (pos, 0.9967)\n"
     ]
    }
   ],
   "source": [
    "words = encode('이게 영화지').split()\n",
    "labels, probs = model_superv.predict(words,k=1)\n",
    "for word, label, prob in zip(words, labels, probs):\n",
    "    word = decode(word)\n",
    "    label = label[0][9:]\n",
    "    prob = float(prob)\n",
    "    print(f'{word}: ({label}, {prob:.4})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "타이포나 띄어쓰기 오류가 포함되어 있더라도 단어의 sentiment 가 판별됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "씹: (neg, 0.9955)\n",
      "이게뭐냐: (neg, 1.0)\n"
     ]
    }
   ],
   "source": [
    "words = encode('씹ㅏㄹ 이게뭐냐').split()\n",
    "labels, probs = model_superv.predict(words,k=1)\n",
    "for word, label, prob in zip(words, labels, probs):\n",
    "    word = decode(word)\n",
    "    label = label[0][9:]\n",
    "    prob = float(prob)\n",
    "    print(f'{word}: ({label}, {prob:.4})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
