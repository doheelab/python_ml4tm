{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovit_textmining_dataset.navernews_10days import get_comments_paths\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "path = get_comments_paths(date='2016-10-20')\n",
    "corpus = DoublespaceLineCorpus(path, iter_sent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word Extractor and Tokenizer\n",
    "\n",
    "명사 추출기를 이용하여 명사 점수를 단어 점수로 이용합니다. 뉴스 기사는 띄어쓰기가 잘 되어 있기 때문에 L-Tokenizer 를 이용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 695989 from 274369 sents. mem=0.200 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2191206, mem=1.948 Gb\n",
      "[Noun Extractor] batch prediction was completed for 167918 words\n",
      "[Noun Extractor] checked compounds. discovered 146262 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 151558 -> 119849\n",
      "[Noun Extractor] postprocessing ignore_features : 119849 -> 119486\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119486 -> 117335\n",
      "[Noun Extractor] 117335 nouns (146262 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=2.344 Gb                    \n",
      "[Noun Extractor] 67.99 % eojeols are covered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['뉴스', '기사', '를', '이용', '하여', '학습', '한', '모델', '입니다']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_extractor = LRNounExtractor_v2()\n",
    "noun_scores = noun_extractor.train_extract(corpus)\n",
    "\n",
    "word_scores = {noun:score.score for noun, score in noun_scores.items()}\n",
    "tokenizer = LTokenizer(scores = word_scores)\n",
    "\n",
    "tokenizer.tokenize('뉴스 기사를 이용하여 학습한 모델입니다', tolerance=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build co-occurrence matrix\n",
    "\n",
    "L parts 에서 명사 점수의 최대값과 0.3 점 차이가 나지 않는다면 그 중 가장 긴 subword 를 단어로 추출하도록 tolerance 를 이용하였습니다. min_tf 를 10 으로 설정하여 10 번 이하로 등장한 단어에 대해서는 co-occurrence 를 계산하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create (word, contexts) matrix\n",
      "  - counting word frequency from 274368 sents, mem=2.533 Gb\n",
      "  - scanning (word, context) pairs from 274368 sents, mem=2.533 Gb\n",
      "  - (word, context) matrix was constructed. shape = (25954, 25954)                    \n",
      "  - done\n"
     ]
    }
   ],
   "source": [
    "from soynlp.vectorizer import sent_to_word_contexts_matrix\n",
    "\n",
    "def custom_tokenizer(sent):\n",
    "    return tokenizer.tokenize(sent, tolerance=0.3)\n",
    "\n",
    "x, idx2vocab = sent_to_word_contexts_matrix(\n",
    "    corpus,\n",
    "    windows = 2,\n",
    "    min_tf = 10,\n",
    "    tokenizer = custom_tokenizer,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 후 30,810 개의 단어가 학습되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25954, 25954)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similar words using context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idx2vocab 을 vocab2idx 로 만듭니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965\n",
      "1780\n"
     ]
    }
   ],
   "source": [
    "vocab2idx = {vocab:idx for idx, vocab in enumerate(idx2vocab)}\n",
    "\n",
    "print(vocab2idx['이화여대'])\n",
    "print(vocab2idx['아이오아이'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.metrics.pairwise_distances 를 이용하여 '이화여대'와 context vector 가 비슷한 다른 단어를 찾습니다. \n",
    "\n",
    "numpy.ndarray 형식인 distance matrix (dist) 에 대하여 argsort() 를 하면, 거리 순서대로 정렬됩니다. sort() 를 하면 값이 정렬되며, argsort() 를 하면 각 값의 index 가 return 됩니다. \n",
    "\n",
    "여러 번 쓸 수 있도록 함수로도 만들어둡니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def most_similar_words(query, wv, topk=10):\n",
    "    \"\"\"\n",
    "    query : str\n",
    "    wv : word representation\n",
    "    topk : num of similar terms\n",
    "    \"\"\"\n",
    "    \n",
    "    if not (query in vocab2idx):\n",
    "        return []\n",
    "\n",
    "    query_idx = vocab2idx[query]\n",
    "    query_vec = wv[query_idx,:].reshape(1,-1)\n",
    "    dist = pairwise_distances(query_vec, wv, metric='cosine')[0]\n",
    "    similars = []\n",
    "\n",
    "    # sorting\n",
    "    for similar_idx in dist.argsort():\n",
    "        \n",
    "        # filtering query term\n",
    "        if similar_idx == query_idx:\n",
    "            continue\n",
    "\n",
    "        if len(similars) >= topk:\n",
    "            break\n",
    "\n",
    "        # decoding index to word\n",
    "        similar_word = idx2vocab[similar_idx]\n",
    "        similars.append((similar_word, 1 - dist[similar_idx]))\n",
    "\n",
    "    return similars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cooccurrence matrix 만을 이용해도 문맥이 **매우** 확실한 단어 `아이오아이`는 다른 아이돌 이름이나 `아오아` 같은 약어가 유사어로 검색됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('블랙핑크', 0.6755289243892053),\n",
       " ('트와이스', 0.622393548554468),\n",
       " ('엑소', 0.6033211536727949),\n",
       " ('우리나라', 0.5597736444192414),\n",
       " ('아오아', 0.5504612405602796),\n",
       " ('새누리', 0.5437366100639845),\n",
       " ('노래', 0.5421394745971377),\n",
       " ('우리', 0.541453581008619),\n",
       " ('에이핑크', 0.5350674956377075),\n",
       " ('김제동씨', 0.5314901989405921)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 단어 `아프리카`의 유사어는 잘 검색되지 않습니다. Co-occurrence frequency 를 representation 으로 이용하면 어느 문맥에나 등장하는 단어들에 영향을 받기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('우리나라', 0.8037611170388616),\n",
       " ('회사', 0.8037363349622527),\n",
       " ('우리', 0.790296534084028),\n",
       " ('머리', 0.787885823985501),\n",
       " ('이나라', 0.7806776760312693),\n",
       " ('정부', 0.7785169302184154),\n",
       " ('스크린도어', 0.7652516200989101),\n",
       " ('여자', 0.7515296898487684),\n",
       " ('트럼프', 0.7500540890560078),\n",
       " ('새누리', 0.7490903619478668)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI 를 이용한 term weighting\n",
    "\n",
    "soynlp 의 pmi 를 이용하여 co-occurrence matrix 에 PMI 를 적용합니다. `min_pmi` 를 0 으로 설정하여 Positive PMI 로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import pmi\n",
    "\n",
    "x_pmi, px, py = pmi(x, min_pmi=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPMI 가 어느 문맥에나 등장하는 단어의 weight 를 0 으로 만들기 때문에 단어의 문맥이 뚜렷해집니다. `아이오아이`의 유사어는 크게 달라지지 않았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ioi', 0.1809293643175891),\n",
       " ('블랙핑크', 0.16269404370236584),\n",
       " ('IOI', 0.15126054480342965),\n",
       " ('블핑', 0.14611345956000332),\n",
       " ('트와이스', 0.1423420888369824),\n",
       " ('아오아', 0.14219738108886726),\n",
       " ('방탄', 0.1324156115896984),\n",
       " ('신곡', 0.12363733671446919),\n",
       " ('컴백', 0.11995624410107986),\n",
       " ('트와', 0.11677553503826488)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 `아프리카`의 유사어는 크게 바뀌었습니다. 다른 나라의 이름도 검색되며, `아프리카tv` 관련 단어인 `bj` 나 `유튜브`가 검색됩니다. 이날 대륙 `아프리카`에 관련된 기사와 `아프리카tv` 관련 기사가 모두 있었기 때문에 두 문맥이 모두 반영되어 있습니다.\n",
    "\n",
    "그러나 representation vector 의 차원은 3 만 차원보다 큽니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bj', 0.09174008479376838),\n",
       " ('비제이', 0.09034566575512504),\n",
       " ('유튜브', 0.08198154421369663),\n",
       " ('유투브', 0.07862526889424482),\n",
       " ('동남아', 0.07464494039211789),\n",
       " ('수수료', 0.07396822530612046),\n",
       " ('있을리', 0.0735855790226736),\n",
       " ('중동', 0.0733190010723227),\n",
       " ('삐지면', 0.069371423755756),\n",
       " ('남미', 0.06922054503695707)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI + SVD 를 이용한 차원 축소\n",
    "\n",
    "Singular Vector Decomposition (SVD) 를 이용하여 차원을 축소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "x_pmisvd = svd.fit_transform(x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD 로 차원 축소를 하면 noise 가 어느 정도 제거됩니다. Co-occurrence matrix 에서의 noise 는 문맥이 특이하거나 infrequent 한 단어입니다. 그 외에는 PPMI 를 적용했을 때와 유사어의 관계는 비슷합니다. 하지만 cosine 유사도의 값은 커집니다. PPMI 만 적용하면 sparse vector 이기 때문에 평균적인 유사도가 작습니다. 하지만 SVD 를 적용하면 평균적인 유사도 scale 이 커집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('블랙핑크', 0.7981585963342795),\n",
       " ('트와이스', 0.7471412848682975),\n",
       " ('블핑', 0.7333447881893075),\n",
       " ('여자친구', 0.7070539576074238),\n",
       " ('ioi', 0.6885774490246201),\n",
       " ('아오아', 0.6850071208237152),\n",
       " ('트와', 0.68421616590016),\n",
       " ('컴백', 0.672958981872358),\n",
       " ('빅뱅', 0.662335156429146),\n",
       " ('신곡', 0.6614188063703947)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x_pmisvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('유튜브', 0.4449765493633768),\n",
       " ('인터넷', 0.43866986095898786),\n",
       " ('유투브', 0.43200091044265876),\n",
       " ('동남아', 0.4208089858095101),\n",
       " ('유럽', 0.4109185784018816),\n",
       " ('선진국', 0.4044908624512855),\n",
       " ('한국사람', 0.4040292067731064),\n",
       " ('국내', 0.4036200429166752),\n",
       " ('중동', 0.4007907540652521),\n",
       " ('TV', 0.39919460520603267)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x_pmisvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S, Sigma, V 를 이용하여 representation 과 mapper 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "def train(X, n_components, n_iter=5, random_state=None):\n",
    "\n",
    "    if (random_state == None) or isinstance(random_state, int):\n",
    "        random_state = check_random_state(random_state)\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    if n_components >= n_features:\n",
    "        raise ValueError(\"n_components must be < n_features;\"\n",
    "                         \" got %d >= %d\" % (n_components, n_features))\n",
    "\n",
    "    U, Sigma, VT = randomized_svd(\n",
    "        X, n_components,\n",
    "        n_iter = n_iter,\n",
    "        random_state = random_state)\n",
    "\n",
    "    S_ = Sigma ** (0.5)\n",
    "    representation = y = U * S_\n",
    "    mapper = (VT.T * S_).T\n",
    "\n",
    "    return representation, mapper\n",
    "\n",
    "wv, mapper = train(x_pmi, n_components=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('블랙핑크', 0.6963338694909909),\n",
       " ('트와이스', 0.6927600045816528),\n",
       " ('ioi', 0.6137406826522469),\n",
       " ('블핑', 0.6072816279453563),\n",
       " ('아오아', 0.5877446138325634),\n",
       " ('여자친구', 0.5770751000180948),\n",
       " ('신곡', 0.5692154765190212),\n",
       " ('트와', 0.5685233305841946),\n",
       " ('엑소', 0.5663855338125494),\n",
       " ('빅뱅', 0.5655052010733637)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('철구', 0.3300399080741523),\n",
       " ('비제이', 0.30671525295552393),\n",
       " ('동남아', 0.3005379534528194),\n",
       " ('유튜브', 0.3002160056698727),\n",
       " ('bj', 0.2994103509084116),\n",
       " ('티비', 0.27348955973975997),\n",
       " ('필리핀', 0.266659702734819),\n",
       " ('ㅅㅍ', 0.26646295692639677),\n",
       " ('중동', 0.2625211073841971),\n",
       " ('시청자', 0.25874905930033076)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
