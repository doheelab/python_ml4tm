{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.493\n",
      "added lovit_textmining_dataset\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "\n",
    "from navernews_10days import get_comments_paths\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "path = get_comments_paths(date='2016-10-20')\n",
    "corpus = DoublespaceLineCorpus(path, iter_sent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word Extractor and Tokenizer\n",
    "\n",
    "명사 추출기를 이용하여 명사 점수를 단어 점수로 이용합니다. 뉴스 기사는 띄어쓰기가 잘 되어 있기 때문에 L-Tokenizer 를 이용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 687560 from 228630 sents. mem=0.198 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2775027, mem=1.670 Gb\n",
      "[Noun Extractor] batch prediction was completed for 193198 words\n",
      "[Noun Extractor] checked compounds. discovered 151319 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 167318 -> 120870\n",
      "[Noun Extractor] postprocessing ignore_features : 120870 -> 120447\n",
      "[Noun Extractor] postprocessing ignore_NJ : 120447 -> 117740\n",
      "[Noun Extractor] 117740 nouns (151319 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=2.027 Gb                    \n",
      "[Noun Extractor] 69.13 % eojeols are covered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['뉴스', '기사', '를', '이용', '하여', '학습', '한', '모델', '입니다']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_extractor = LRNounExtractor_v2()\n",
    "noun_scores = noun_extractor.train_extract(corpus)\n",
    "\n",
    "word_scores = {noun:score.score for noun, score in noun_scores.items()}\n",
    "tokenizer = LTokenizer(scores = word_scores)\n",
    "\n",
    "tokenizer.tokenize('뉴스 기사를 이용하여 학습한 모델입니다', tolerance=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build co-occurrence matrix\n",
    "\n",
    "L parts 에서 명사 점수의 최대값과 0.3 점 차이가 나지 않는다면 그 중 가장 긴 subword 를 단어로 추출하도록 tolerance 를 이용하였습니다. min_tf 를 10 으로 설정하여 10 번 이하로 등장한 단어에 대해서는 co-occurrence 를 계산하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create (word, contexts) matrix\n",
      "  - counting word frequency from 228629 sents, mem=2.172 Gb\n",
      "  - scanning (word, context) pairs from 228629 sents, mem=2.172 Gb\n",
      "  - (word, context) matrix was constructed. shape = (30618, 30618)                    \n",
      "  - done\n"
     ]
    }
   ],
   "source": [
    "from soynlp.vectorizer import sent_to_word_contexts_matrix\n",
    "\n",
    "def custom_tokenizer(sent):\n",
    "    return tokenizer.tokenize(sent, tolerance=0.3)\n",
    "\n",
    "x, idx2vocab = sent_to_word_contexts_matrix(\n",
    "    corpus,\n",
    "    windows = 2,\n",
    "    min_tf = 10,\n",
    "    tokenizer = custom_tokenizer,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 후 30,810 개의 단어가 학습되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30618, 30618)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similar words using context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idx2vocab 을 vocab2idx 로 만듭니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340\n",
      "1984\n"
     ]
    }
   ],
   "source": [
    "vocab2idx = {vocab:idx for idx, vocab in enumerate(idx2vocab)}\n",
    "\n",
    "print(vocab2idx['이화여대'])\n",
    "print(vocab2idx['아이오아이'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.metrics.pairwise_distances 를 이용하여 '이화여대'와 context vector 가 비슷한 다른 단어를 찾습니다. \n",
    "\n",
    "numpy.ndarray 형식인 distance matrix (dist) 에 대하여 argsort() 를 하면, 거리 순서대로 정렬됩니다. sort() 를 하면 값이 정렬되며, argsort() 를 하면 각 값의 index 가 return 됩니다. \n",
    "\n",
    "여러 번 쓸 수 있도록 함수로도 만들어둡니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def most_similar_words(query, wv, topk=10):\n",
    "    \"\"\"\n",
    "    query : str\n",
    "    wv : word representation\n",
    "    topk : num of similar terms\n",
    "    \"\"\"\n",
    "    \n",
    "    if not (query in vocab2idx):\n",
    "        return []\n",
    "\n",
    "    query_idx = vocab2idx[query]\n",
    "    query_vec = wv[query_idx,:].reshape(1,-1)\n",
    "    dist = pairwise_distances(query_vec, wv, metric='cosine')[0]\n",
    "    similars = []\n",
    "\n",
    "    # sorting\n",
    "    for similar_idx in dist.argsort():\n",
    "        \n",
    "        # filtering query term\n",
    "        if similar_idx == query_idx:\n",
    "            continue\n",
    "\n",
    "        if len(similars) >= topk:\n",
    "            break\n",
    "\n",
    "        # decoding index to word\n",
    "        similar_word = idx2vocab[similar_idx]\n",
    "        similars.append((similar_word, 1 - dist[similar_idx]))\n",
    "\n",
    "    return similars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cooccurrence matrix 만을 이용해도 문맥이 **매우** 확실한 단어 `아이오아이`는 다른 아이돌 이름이나 `아오아` 같은 약어가 유사어로 검색됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('블랙핑크', 0.6581659649921858),\n",
       " ('트와이스', 0.6258572495971212),\n",
       " ('엑소', 0.5828096369899102),\n",
       " ('여자친구', 0.5788681011489352),\n",
       " ('노래', 0.5463866900815891),\n",
       " ('우리나라', 0.5435773641801577),\n",
       " ('아오아', 0.5379811668561362),\n",
       " ('우리', 0.5273834320883796),\n",
       " ('가수', 0.5266958156561162),\n",
       " ('새누리', 0.5220714249424844)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 단어 `아프리카`의 유사어는 잘 검색되지 않습니다. Co-occurrence frequency 를 representation 으로 이용하면 어느 문맥에나 등장하는 단어들에 영향을 받기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('회사', 0.8037971762424634),\n",
       " ('우리나라', 0.7900979733733253),\n",
       " ('머리', 0.78224110041578),\n",
       " ('우리', 0.7763852370588487),\n",
       " ('이나라', 0.764068648599605),\n",
       " ('정부', 0.7632069770872995),\n",
       " ('학교', 0.7615474449969353),\n",
       " ('너네', 0.7518707053229157),\n",
       " ('스크린도어', 0.7501438876269355),\n",
       " ('새누리', 0.7406757432943758)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI 를 이용한 term weighting\n",
    "\n",
    "soynlp 의 pmi 를 이용하여 co-occurrence matrix 에 PMI 를 적용합니다. `min_pmi` 를 0 으로 설정하여 Positive PMI 로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import pmi\n",
    "\n",
    "x_pmi, px, py = pmi(x, min_pmi=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPMI 가 어느 문맥에나 등장하는 단어의 weight 를 0 으로 만들기 때문에 단어의 문맥이 뚜렷해집니다. `아이오아이`의 유사어는 크게 달라지지 않았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ioi', 0.17973380445809284),\n",
       " ('블랙핑크', 0.16560387184185066),\n",
       " ('트와이스', 0.15580021478108652),\n",
       " ('아오아', 0.14810905975425448),\n",
       " ('블핑', 0.1361151825149176),\n",
       " ('너무너무너무', 0.13231819811777146),\n",
       " ('CLC', 0.12497139230114185),\n",
       " ('엑소', 0.12160576569048331),\n",
       " ('방탄', 0.11965180115621088),\n",
       " ('트와', 0.11833934686696168)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 `아프리카`의 유사어는 크게 바뀌었습니다. 다른 나라의 이름도 검색되며, `아프리카tv` 관련 단어인 `bj` 나 `유튜브`가 검색됩니다. 이날 대륙 `아프리카`에 관련된 기사와 `아프리카tv` 관련 기사가 모두 있었기 때문에 두 문맥이 모두 반영되어 있습니다.\n",
    "\n",
    "그러나 representation vector 의 차원은 3 만 차원보다 큽니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('비제이', 0.08428223905858767),\n",
       " ('bj', 0.07921126656182187),\n",
       " ('수수료', 0.0742366382647287),\n",
       " ('중동', 0.07224957662291298),\n",
       " ('유튜브', 0.06996363416062701),\n",
       " ('플랫폼', 0.06782625167091183),\n",
       " ('대도서관', 0.06712023146678547),\n",
       " ('벌어들', 0.06647189913183482),\n",
       " ('유투브', 0.06417289761041922),\n",
       " ('클린', 0.061716503788260324)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI + SVD 를 이용한 차원 축소\n",
    "\n",
    "Singular Vector Decomposition (SVD) 를 이용하여 차원을 축소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "x_pmisvd = svd.fit_transform(x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD 로 차원 축소를 하면 noise 가 어느 정도 제거됩니다. Co-occurrence matrix 에서의 noise 는 문맥이 특이하거나 infrequent 한 단어입니다. 그 외에는 PPMI 를 적용했을 때와 유사어의 관계는 비슷합니다. 하지만 cosine 유사도의 값은 커집니다. PPMI 만 적용하면 sparse vector 이기 때문에 평균적인 유사도가 작습니다. 하지만 SVD 를 적용하면 평균적인 유사도 scale 이 커집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('블랙핑크', 0.7965188797621157),\n",
       " ('트와이스', 0.7951284625715093),\n",
       " ('블핑', 0.7740097675003359),\n",
       " ('트와', 0.7493663315577943),\n",
       " ('엑소', 0.7408083642013694),\n",
       " ('ioi', 0.7250002122971044),\n",
       " ('빅뱅', 0.7234435047012071),\n",
       " ('아오아', 0.7097597687481514),\n",
       " ('여자친구', 0.704434021408181),\n",
       " ('방탄', 0.6874623193165361)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x_pmisvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('유투브', 0.4899624680949647),\n",
       " ('유튜브', 0.4607744719229442),\n",
       " ('bj', 0.45474446236500043),\n",
       " ('프로그램', 0.43869881299333),\n",
       " ('한계', 0.43799992904494467),\n",
       " ('중소기업', 0.4284874079048935),\n",
       " ('후진국', 0.4225473546074232),\n",
       " ('동남아', 0.42075192796849603),\n",
       " ('이슬람', 0.4187904041774626),\n",
       " ('이미지', 0.4114119571148269)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x_pmisvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S, Sigma, V 를 이용하여 representation 과 mapper 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "def train(X, n_components, n_iter=5, random_state=None):\n",
    "\n",
    "    if (random_state == None) or isinstance(random_state, int):\n",
    "        random_state = check_random_state(random_state)\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    if n_components >= n_features:\n",
    "        raise ValueError(\"n_components must be < n_features;\"\n",
    "                         \" got %d >= %d\" % (n_components, n_features))\n",
    "\n",
    "    U, Sigma, VT = randomized_svd(\n",
    "        X, n_components,\n",
    "        n_iter = n_iter,\n",
    "        random_state = random_state)\n",
    "\n",
    "    S_ = Sigma ** (0.5)\n",
    "    representation = y = U * S_\n",
    "    mapper = (VT.T * S_).T\n",
    "\n",
    "    return representation, mapper\n",
    "\n",
    "wv, mapper = train(x_pmi, n_components=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('트와이스', 0.7562570038775457),\n",
       " ('블랙핑크', 0.7542341700254398),\n",
       " ('엑소', 0.679682713038533),\n",
       " ('트와', 0.6700636725186646),\n",
       " ('블핑', 0.6696587966066099),\n",
       " ('ioi', 0.6506407392295206),\n",
       " ('빅뱅', 0.6324141740977194),\n",
       " ('여자친구', 0.6268380730416317),\n",
       " ('아오아', 0.6158338023198218),\n",
       " ('유닛', 0.6025530918570341)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('유투브', 0.3864460957574195),\n",
       " ('티비', 0.35121599517166446),\n",
       " ('bj', 0.33680517731984305),\n",
       " ('유튜브', 0.3335360050218502),\n",
       " ('비제이', 0.3188627913806349),\n",
       " ('중동', 0.30562114059481327),\n",
       " ('동남아', 0.297176342770448),\n",
       " ('시청자', 0.28850889710176764),\n",
       " ('tv', 0.2873335460623294),\n",
       " ('이슬람', 0.2803188144537745)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
