{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.7.x 에서 뉴스의 본문일부가 파싱이 제대로 되지 않는 문제가 있습니다. 이는 Python 3.6.x 에서 작성하였습니다.\n",
    "\n",
    "이전의 네이버 영화 스크랩 코드에서 작성한 get_soup 과 normalize 함수를 utils.py 에 넣어뒀습니다. 이를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version = sys.version_info(major=3, minor=6, micro=2, releaselevel='final', serial=0)\n",
      "BeautifulSoup version = 4.7.1\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from utils import get_soup\n",
    "from utils import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 링크의 뉴스 기사를 가져와 제목과 본문을 가져옵니다. 이는 앞서 연습한 것처럼 inspect 를 이용하여 HTML 태그를 살펴보면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=102&oid=422&aid=0000370702'\n",
    "soup = get_soup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"강남경찰서 또 비위…피의자에게 금품수수 '입건'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_title(soup):\n",
    "    return normalize(soup.select('h3[id=articleTitle]')[0].text)\n",
    "\n",
    "parse_title(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "때로 br 태그를 이용하여 줄바꿈을 하는 경우들이 있는데, 이 정보를 살리기 위해 br 태그를 str 로 변환하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_content(soup):\n",
    "    html = str(soup.select(\"div[id=articleBodyContents]\")[0])\n",
    "    html = html.replace('<br/>', 'br/')\n",
    "    content = BeautifulSoup(html, 'lxml').text\n",
    "    content = '  '.join(normalize(s) for s in content.split('br/') if s.strip())\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_content(soup).split('  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드를 정리하면 scrap_news 라는 함수를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_news(url):\n",
    "    json_obj = {'url': url}\n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "        json_obj['title'] = parse_title(soup)\n",
    "        json_obj['content'] = parse_content(soup)\n",
    "        return json_obj\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# scrap_news(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색어를 입력하여 뉴스 기사들의 urls 을 가져오기 위한 함수도 만들어 봅니다. URL 에는 한글이 입력될 수 없습니다. 이들은 다른 글자로 인코딩이 되어야 합니다. URL 에 한글이 입력되어 있다면 이를 복사하여 다른 메모장에 붙여보시면 16진법으로 기술된 글자가 나옵니다. 한글이 16진법 숫자로 변환된 것인데, urllib 에서 이러한 기능을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def get_search_url(query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    query : str\n",
    "        질의어\n",
    "    start_date : str\n",
    "        yyyy.mm.dd 형식 ex) 2019.04.18\n",
    "    end_date : str\n",
    "        yyyy.mm.dd 형식 ex) 2019.04.18\n",
    "    \"\"\"\n",
    "    base = 'https://search.naver.com/search.naver?where=news&query={0}&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={2}'\n",
    "    url = base.format(url_encode(query), start_date, end_date)\n",
    "    return url\n",
    "\n",
    "def url_encode(query, encoding='utf-8'):\n",
    "    def encode_a_term(term):\n",
    "        try:\n",
    "            return urllib.parse.quote(term, encoding=encoding)\n",
    "        except Exception as e:\n",
    "            raise ValueError('Failed to encode query %s' % str(e))\n",
    "    return '+'.join([encode_a_term(term) for term in query.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%EB%B2%84%EB%8B%9D%EC%8D%AC'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_encode('버닝썬')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_encode 함수를 이용하여 검색어, 시작일, 종료일이 입력되었을 때 뉴스 검색 결과를 가져오는 url 을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://search.naver.com/search.naver?where=news&query=%EB%B2%84%EB%8B%9D%EC%8D%AC&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2019.04.18&de=2019.04.19'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = get_search_url('버닝썬', '2019.04.18', '2019.04.19')\n",
    "base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스가 몇 건 검색되었는지 숫자를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_article_num(url):\n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "        if not soup:\n",
    "            return 0\n",
    "        header_text = soup.select('div[class=section_head] div[class^=title_desc] span')\n",
    "        if not header_text:\n",
    "            return 0\n",
    "        header_text = header_text[0].text\n",
    "        header_text = re.findall('[,\\\\d]+건', header_text)[0]\n",
    "        header_text = re.sub(',', '', header_text) # Remove Comma\n",
    "        num_articles = int(header_text[:-1])\n",
    "        return num_articles\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError('Failed to get total number of articles %s' % str(e))\n",
    "\n",
    "get_article_num(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 패턴이 있는 링크만 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_urls(base_url, page):\n",
    "\n",
    "    url_patterns = ('a[href^=\"https://news.naver.com/main/read.nhn?\"]',\n",
    "            'a[href^=\"https://entertain.naver.com/main/read.nhn?\"]',\n",
    "            'a[href^=\"https://sports.news.naver.com/sports/index.nhn?\"]',\n",
    "            'a[href^=\"https://news.naver.com/sports/index.nhn?\"]')\n",
    "\n",
    "    urls_in_page = set()\n",
    "    page_url = '{}&start={}&refresh_start=0'.format(base_url, 1 + 10*(page-1))\n",
    "    soup = get_soup(page_url)\n",
    "    if not soup:\n",
    "        return urls_in_page\n",
    "    try:\n",
    "        article_blocks = soup.select('ul[class=type01]')[0]\n",
    "        for pattern in url_patterns:\n",
    "            article_urls = [link['href'] for link in article_blocks.select(pattern)]\n",
    "            urls_in_page.update(article_urls)\n",
    "    except Exception as e:\n",
    "        raise ValueError('Failed to extract urls from page %s' % str(e))\n",
    "\n",
    "    return urls_in_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색 결과로부터 뉴스 기사의 링크를 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting urls from page = 1\n",
      "getting urls from page = 2\n",
      "getting urls from page = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "num_article = get_article_num(url)\n",
    "num_article = 30 # debug\n",
    "base_url = get_search_url('버닝썬', '2019.04.18', '2019.04.19')\n",
    "\n",
    "urls = []\n",
    "for page in range(1, math.ceil(num_article / 10) + 1):\n",
    "    urls += get_article_urls(base_url, page)    \n",
    "    print('getting urls from page = {}'.format(page))\n",
    "    time.sleep(1)\n",
    "\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "댓글의 경우에는 크롬의 Network 를 이용하여 request url 을 가져올 수 있습니다. 여기에는 header 에 입력되는 user-agent 와 referer 의 값도 함께 기록되어 있습니다. 뉴스 기사의 url 이 referer 입니다. 이를 이용하여 API 주소를 가져옵니다.\n",
    "\n",
    "![](news_comments.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "referer_url = 'https://news.naver.com/main/read.nhn?m_view=1&mode=LSD&mid=shm&sid1=100&oid=008&aid=0004206900'\n",
    "request_url = 'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?ticket=news&templateId=default_politics&pool=cbox5&_callback=jQuery112406445509904006934_1555720412273&lang=ko&country=KR&objectId=news008%2C0004206900&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=2&refresh=false&sort=NEW&current=1700689291&prev=1700692531&includeAllStatus=true&_=1555720412275'\n",
    "headers = {'Referer': referer_url}\n",
    "\n",
    "r = requests.get(request_url, headers=headers)\n",
    "text = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response 의 시작과 끝에는 java script 관련 단어들이 포함되어 있습니다. 이를 제거한 뒤, JSON parsing 을 합니다. json.load 는 파일을 JSON 형식으로 읽는 함수이며, json.loads 는 str 을 JSON 형식으로 읽는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "text = text[text.index('(')+1:-2]\n",
    "response = json.loads(text)\n",
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 정리하면 한 뉴스기사의 댓글을 가져오는 get_response 함수를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(request_url, referer_url):\n",
    "    r = requests.get(request_url, headers=headers)\n",
    "    text = r.text\n",
    "    text = text[text.index('(')+1:-2]\n",
    "    return json.loads(text)\n",
    "\n",
    "comments = get_response(request_url, referer_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['success', 'code', 'message', 'lang', 'country', 'result', 'date'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
