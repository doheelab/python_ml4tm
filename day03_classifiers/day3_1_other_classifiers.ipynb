{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 종류의 classifiers 의 사용법을 익히기 위하여 앞서 다뤘던 영화 평점 분류 데이터를 이용합니다. Support Vector Machine 계열들은 데이터를 그대로 외우기 때문에 데이터의 크기가 커지면 모델의 크기도 커집니다. 그렇기 때문에 이번 실습에서는 데이터의 개수가 10k 인 작은 데이터를 이용합니다. 긍정 (label = 1), 부정 (label = -1) 의 리뷰 5k 씩으로 구성되어 있는 balanced dataset 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.3\n",
      "(10000, 4808)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import warnings\n",
    "from lovit_textmining_dataset.navermovie_comments import load_sentiment_dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(sklearn.__version__)\n",
    "\n",
    "texts, x, y, idx_to_vocab = load_sentiment_dataset(data_name='10k', tokenize='komoran')\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize\n",
    "\n",
    "학습용 데이터 x는 row normalize를 미리 해두겠습니다. 기본은 norm='l2' 입니다. 벡터를 확률의 형식으로 (합이 1 이 되도록) 만들고 싶다면 norm 을 'l1' 으로 변경하면 됩니다.\n",
    "\n",
    "```\n",
    "x_norm_l1 = normalize(x, norm='l1')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "x_norm = normalize(x, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "모델의 사용법을 알기 위함이니 학습과 검증 데이터를 따로 분류하지는 않겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.78 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(x_norm, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트는 여러 classifier 를 이용하여 반복할테니 함수로 만들어 둡니다. 학습 정확도를 기준으로 모델의 학습력을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.8745\n"
     ]
    }
   ],
   "source": [
    "def test(classifier, x, y):\n",
    "    y_pred = classifier.predict(x)\n",
    "    accuracy = (y == y_pred).mean()\n",
    "    print(f'training accuracy = {accuracy:.4}')\n",
    "\n",
    "test_samples = np.asarray(list(range(0,10)) + list(range(5000, 5010)))\n",
    "test(logistic_regression, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression의 coefficient를 살펴보는 것은 이전에 다뤘습니다. 긍정, 부정을 의미하는 단어들을 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관람객/NNG : 7.07\n",
      "놀라/VV : 4.77\n",
      "최고/NNG : 4.43\n",
      "10/SN : 3.81\n",
      "인생/NNP : 3.46\n",
      "필요/NNG : 3.3\n",
      "롭/XSA : 3.04\n",
      "재미있/VA : 3.02\n",
      "우주/NNG : 2.96\n",
      "인터스텔라/NNP : 2.87\n"
     ]
    }
   ],
   "source": [
    "# Coefficients를 array로 만듭니다. \n",
    "coefficients = logistic_regression.coef_.reshape(-1)\n",
    "\n",
    "# Coefficient를 (index, coef)로 만든 뒤, coefficient 의 크기로 내림차순 정렬합니다. \n",
    "sorted_coefficients = sorted(enumerate(coefficients), key=lambda x:-x[1])\n",
    "\n",
    "# 긍정적인 리뷰에 자주 등장하는 단어들을 다시 살펴봅니다.\n",
    "for j, coef in sorted_coefficients[:10]:\n",
    "    print('{} : {:.3}'.format(idx_to_vocab[j], coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Classifier\n",
    "\n",
    "Feedforward neural network 의 sklearn 의 이름이며, NN 은 parameter 를 해석하기가 조금 복잡합니다. 하지만 학습 방법은 Logistic Regression 과 동일합니다. \n",
    "\n",
    "Neural network 의 경우, hidden laber 의 층 수와, 각 층의 노드 수를 정해줄 수 있습니다. 아래 예시처럼 hidden layer 를 정하면, 2 개의 layers 에 각각 20, 5 개의 nodes 를 이용한다는 의미입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.9795\n",
      "CPU times: user 1min 19s, sys: 3min 32s, total: 4min 52s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "neural_network = MLPClassifier(hidden_layer_sizes=(20,5))\n",
    "neural_network.fit(x_norm, y)\n",
    "\n",
    "test(neural_network, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients 는 coefs_ 에 list 형식으로 저장됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(type(neural_network.coefs_))\n",
    "print(len(neural_network.coefs_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 : (4808, 20)\n",
      "layer 2 : (20, 5)\n",
      "layer 3 : (5, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, coef_ in enumerate(neural_network.coefs_):\n",
    "    print('layer {} : {}'.format(i+1, coef_.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intercept 는 $h_{t-1} \\times \\beta_t + intercept_t$ 처럼 이전 layer 의 output 값과 $\\beta$ 의 내적에 더해지는 값입니다. Logistic regression 에서의 평행이동과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.09337455,  0.10501489,  0.05272212,  0.06463424,  0.05581306,\n",
       "         0.34889031,  0.06916448,  0.30141196, -0.18363755,  0.05200041,\n",
       "         0.1156127 ,  0.14034019,  0.11764886,  0.33767206,  0.05829439,\n",
       "         0.09752739,  0.1578528 ,  0.08662141,  0.06460719,  0.09404335]),\n",
       " array([0.38472889, 0.32938485, 0.10315762, 0.53241528, 0.24371127]),\n",
       " array([0.42249033])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Classifier)\n",
    "\n",
    "SVM은 kernel을 정할 수 있습니다. default는 linear입니다. \n",
    "\n",
    "Linear kernel이 속도도 훨씬 빠릅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.774\n",
      "CPU times: user 25.6 s, sys: 84 ms, total: 25.7 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rbf_SVM = SVC(C=1.0, kernel='rbf')\n",
    "rbf_SVM.fit(x_norm, y)\n",
    "test(rbf_SVM, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rbf_SVM.support_vectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 경우는 저도 자주 보지 못합니다만, 데이터를 모두 외워버렸습니다. k-nearest neighbor 와 다를 게 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4808)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_SVM.support_vectors_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.9088\n",
      "CPU times: user 10.3 s, sys: 32 ms, total: 10.3 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "linear_SVM = SVC(C=1.0, kernel='linear')\n",
    "linear_SVM.fit(x_norm, y)\n",
    "test(linear_SVM, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 은 support vector를 직접 확인할 수가 있습니다. \n",
    "\n",
    "linear SVM 의 support vector들을 확인해 보겠습니다. (4167, 4808) 은 4,808 차원 (단어 개수) 의 4,167 개의 support vectors 가 있다는 의미입니다. 이 숫자도 매우 큽니다. 이는 features 가 반복되지 않는 (각 리뷰별로 다양한 단어들이 이용되는) 데이터셋이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4167, 4808)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_SVM.support_vectors_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 데이터 X 에서의 각 support vectors 의 index 가 support_ 에 저장되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 9989, 9994, 9996], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_SVM.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC.dual\\_coef\\_에는 alpha가 학습되어 있습니다. 이 값이 negative이면 negative class 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4167 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4167 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_SVM.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hist, bin_edges = np.histogram(\n",
    "    linear_SVM.dual_coef_.data,\n",
    "    bins=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 support vectors 의 weight $\\alpha$ 의 값이 -1 아니면 1 에 가깝습니다. 사실상 대부분의 리뷰들이 비슷한 중요도를 지닙니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.0 ~ -0.9) : 1637\n",
      "(-0.9 ~ -0.8) : 56\n",
      "(-0.8 ~ -0.7) : 45\n",
      "(-0.7 ~ -0.6) : 61\n",
      "(-0.6 ~ -0.5) : 43\n",
      "(-0.5 ~ -0.4) : 64\n",
      "(-0.4 ~ -0.3) : 59\n",
      "(-0.3 ~ -0.2) : 52\n",
      "(-0.2 ~ -0.1) : 56\n",
      "(-0.1 ~ 0.0) : 54\n",
      "(0.0 ~ 0.1) : 33\n",
      "(0.1 ~ 0.2) : 34\n",
      "(0.2 ~ 0.3) : 33\n",
      "(0.3 ~ 0.4) : 47\n",
      "(0.4 ~ 0.5) : 45\n",
      "(0.5 ~ 0.6) : 35\n",
      "(0.6 ~ 0.7) : 31\n",
      "(0.7 ~ 0.8) : 39\n",
      "(0.8 ~ 0.9) : 48\n",
      "(0.9 ~ 1.0) : 1695\n"
     ]
    }
   ],
   "source": [
    "for b, e, hist_i in zip(bin_edges, bin_edges[1:], hist):\n",
    "    print('({:.3} ~ {:.3}) : {}'.format(b, e, hist_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "naive bayes 역시 학습 속도가 매우 빠릅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.8922\n",
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 19.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoulli_naive_bayes = BernoulliNB()\n",
    "bernoulli_naive_bayes.fit(x_norm, y)\n",
    "test(bernoulli_naive_bayes, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes 역시 binary classification 에서는 (1, n_terms) 의 coefficient 가 저장되어 있습니다. 숫자가 클수록 긍정 리뷰에 발생하는 단어라는 의미입니다.\n",
    "\n",
    "일반적으로 확률의 곲을 이용하는 모델들은 계산의 편의성을 위하여 log 확률을 이용합니다. 확률의 누적곲에 log 를 취하면 덧샘이 되기 때문입니다. Naive Bayes 의 coefficient 의 값은 log 가 취해진 확률값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.57168296, -8.51759311, -4.32793837, ..., -5.95264375,\n",
       "        -7.13129875, -5.33953928]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_naive_bayes.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 Naive Bayes 는 단어 빈도수 정보까지 coefficient 에 저장합니다. Classsification 에 불필요한 단어가 많이 포함되어 있다면 이들에 의해 classification 의 성능이 저하될 수도 있습니다. 즉 Naive Bayes 를 잘 이용하기 위해서는 정보력이 적은 (frequent) 단어들은 제거해 주면 도움이 됩니다.\n",
    "\n",
    "그리고 판별에 명확한 힌트가 되는 단어들로만 구성된 데이터셋이라면 coefficient 의 해석도 자연스럽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관람객/NNG : -0.901\n",
      "ㄴ/ETM : -1.07\n",
      "보/VV : -1.13\n",
      "영화/NNG : -1.16\n",
      "이/VCP : -1.3\n",
      "다/EC : -1.35\n",
      "하/XSV : -1.44\n",
      "는/ETM : -1.45\n",
      "고/EC : -1.51\n",
      "이/JKS : -1.52\n"
     ]
    }
   ],
   "source": [
    "# Coefficients를 array로 만듭니다. \n",
    "coefficients = bernoulli_naive_bayes.coef_.reshape(-1)\n",
    "\n",
    "# Coefficient를 (index, coef)로 만든 뒤, coefficient 의 크기로 내림차순 정렬합니다. \n",
    "sorted_coefficients = sorted(enumerate(coefficients), key=lambda x:-x[1])\n",
    "\n",
    "for j, coef in sorted_coefficients[:10]:\n",
    "    print('{} : {:.3}'.format(idx_to_vocab[j], coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.8103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    max_depth=50,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    class_weight=None\n",
    ")\n",
    "\n",
    "decision_tree.fit(x_norm, y)\n",
    "test(decision_tree, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.787\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    class_weight=None\n",
    ")\n",
    "\n",
    "decision_tree.fit(x_norm, y)\n",
    "test(decision_tree, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,  161,  162,  178,  179,  184,  206,  257,  336,  370,  480,\n",
       "        494,  559,  585,  842,  877,  882,  931,  985, 1006, 1018, 1031,\n",
       "       1032, 1074, 1111, 1152, 1255, 1299, 1406, 1557, 1591, 1662, 2078,\n",
       "       2114, 2512, 2601, 2716, 2734, 2741, 2808, 2851, 2983, 2996, 3098,\n",
       "       3359, 3368, 3408, 3435, 3551, 3697, 3706, 3876, 3947, 3973, 3998,\n",
       "       4000, 4008, 4176, 4213, 4456, 4511, 4517, 4518, 4558])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(decision_tree.feature_importances_ > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4808,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇 개의 단어만을 이용하여 분류를 하다보니 분류가 제대로 이뤄지지 않기도 합니다. 게다가 분류에 유용한 features 가 아닌 경우도 많습니다. 또한 `크리스토퍼 놀란` 때문에 `놀라/VV`, `놀라/NNP` 와 같은 잘못된 features 들도 생성되었음을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('관람객/NNG', 0.451064896067875),\n",
       " ('놀라/VV', 0.10633635178663352),\n",
       " ('최고/NNG', 0.08842323250056373),\n",
       " ('번/NNB', 0.038251594698815006),\n",
       " ('경이/NNG', 0.034499105314828776),\n",
       " ('필요/NNG', 0.029883137110439176),\n",
       " ('수/NNB', 0.028917042393979457),\n",
       " ('놀라/NNP', 0.027173280668945796),\n",
       " ('아깝/VA', 0.023679729407711188),\n",
       " ('인터스텔라/NNP', 0.020905808382841992)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(idx_to_vocab[idx], importancy) for idx, importancy in \n",
    " sorted(\n",
    "     filter(lambda x:x[1]>0,\n",
    "            enumerate(decision_tree.feature_importances_)),\n",
    "     key=lambda x:-x[1]\n",
    " )\n",
    "][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.7816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    min_samples_leaf=10,\n",
    "    max_depth=4,\n",
    "    max_features=16 # or ['sqrt', 'log2', None] if None then x_norm.shape[1]\n",
    ")\n",
    "\n",
    "random_forest.fit(x_norm, y)\n",
    "test(random_forest, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimators 에 각각의 decision tree 가 저장되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_forest.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 확인해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "                       max_features=16, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=10, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=332332089, splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.estimators_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 decision tree 역시 classifier 로 이용 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy = 0.4981\n"
     ]
    }
   ],
   "source": [
    "test(random_forest.estimators_[0], x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 개씩 decision tree 를 추가할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# incrementally add new random classifiers\n",
    "for _ in range(5):\n",
    "    random_forest.set_params(\n",
    "        n_estimators = random_forest.n_estimators + 100,\n",
    "        warm_start = True\n",
    "    )\n",
    "    random_forest.fit(x_norm, y)\n",
    "    print(len(random_forest.estimators_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_proba 함수를 이용하면 각 instance 가 클래스에 속할 확률을 return 합니다. 아래와 같은 결과는 거의 대부분의 확률이 0.5 에 가깝습니다. 이는 데이터를 분류하는 경계가 뚜렷하지 않다는 의미이기도 합니다. 앙상블 모델을 구성하는 base model 들이 weak classifiers 이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5032677 , 0.4967323 ],\n",
       "       [0.50828244, 0.49171756],\n",
       "       [0.50236723, 0.49763277],\n",
       "       [0.50641861, 0.49358139],\n",
       "       [0.49371089, 0.50628911],\n",
       "       [0.52492934, 0.47507066],\n",
       "       [0.50942032, 0.49057968],\n",
       "       [0.50474634, 0.49525366],\n",
       "       [0.50831127, 0.49168873],\n",
       "       [0.50244556, 0.49755444]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.predict_proba(x_norm[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 모든 학습 데이터에 대하여 예측 확률의 평균을 계산하면 0.51 을 조금 넘습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5122370033309187"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.predict_proba(x_norm).max(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
