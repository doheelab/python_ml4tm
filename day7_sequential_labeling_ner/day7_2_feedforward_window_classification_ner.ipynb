{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "뉴스기사에서 정치인 혹은 직책명과 연예인 이름을 찾는 window classification 기반 모델을 만들어 봅니다. 앞서 Logistic regression 을 이용할 때에는 sparse vector 로 context words 를 입력하였습니다. 이 부분을 Word2Vec 의 벡터로 변환하여 입력합니다.\n",
    "\n",
    "토크나이징을 위하여 명사 추출을 한 뒤, L-Tokenizer 로 명사 부분을 어절에서 잘라내었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.492\n",
      "added lovit_textmining_dataset\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 403896 from 223357 sents. mem=0.226 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4434442, mem=0.954 Gb\n",
      "[Noun Extractor] batch prediction was completed for 119705 words\n",
      "[Noun Extractor] checked compounds. discovered 70639 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 109312 -> 92205\n",
      "[Noun Extractor] postprocessing ignore_features : 92205 -> 91999\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91999 -> 90643\n",
      "[Noun Extractor] 90643 nouns (70639 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.146 Gb                    \n",
      "[Noun Extractor] 76.63 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from navernews_10days import get_news_paths\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "path = get_news_paths(date='2016-10-20')\n",
    "sents = list(DoublespaceLineCorpus(path, iter_sent=True))\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(sents)\n",
    "noun_score = {noun:score.score for noun, score in nouns.items()}\n",
    "\n",
    "tokenizer = LTokenizer(scores=noun_score)\n",
    "sents = [tokenizer.tokenize(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 을 이용하여 명사와 그 외의 어절들의 word vector 를 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sents)\n",
    "\n",
    "wv = word2vec.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 은 infrequent words 에 대해서는 단어 벡터가 잘 학습되지 않습니다. 35000 번째 단어의 빈도수가 10 이니 이 단어까지만 이용합니다. 학습이 잘 된 단어 벡터에 대한 기준은 이 블로그를 참고하시기 바랍니다.\n",
    "\n",
    "https://lovit.github.io/nlp/representation/2018/12/05/min_count_of_word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vocab(count:10, index:35000, sample_int:4294967296)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(word2vec.wv.vocab[word2vec.wv.index2word[35000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알려지지 않은 단어에 대해서는 모두 zero vector 를 할당합니다. 35001 번째 단어는 unknown 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ = np.vstack([wv[:35000], np.zeros((1, wv.shape[1]))])\n",
    "vocab_to_idx = {vocab:idx for idx, vocab in enumerate(word2vec.wv.index2word[:35000])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 모델의 유사어 검색 기능을 이용하여 seed words 로 이용할 단어를 선정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda3/envs/pytorch/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "seed_words = {w for w, _ in word2vec.wv.most_similar('김무성', topn=50)}\n",
    "seed_words.update({w for w, _ in word2vec.wv.most_similar('노무현', topn=50)})\n",
    "seed_words = set(\n",
    "    '''4선 강석진 강석호 과학기술계 권석창 권은희 김관영 김기선 김대중 김동철\n",
    "    김만복 김명연 김성식 김영호 김용국 김정우 김정일 남재준 노무현정부 대구시의회\n",
    "    대변인 대북인권 미셰우 민화 박근혜 박덕흠 박정희 박주민 박주선 박찬대 백종천\n",
    "    버락 부시 비대위 비서실장 상근부회장 서청원 송원영 송하진 아베 아키노 안보실장\n",
    "    연설기획비서관 오바마 유동수 이낙연 이명박 이장우 이재정 이정훈 이종걸 이종배'''.split())\n",
    "#    연설기획비서관 오바마 유동수 이낙연 이명박 이장우 이재정 이정훈 이종걸 이종배\n",
    "#    인수위원회 임종성 전두환 전북도지사 정우택 지우마 진선미 최연혜 충북경제자유구역청\n",
    "#    충북도의회 테메르 통일부장관 호세프 황광 황주홍 후세인'''.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seed_words` 가 하나라도 포함된 문장만 학습에 이용합니다. 그 외의 문장을 모두 이용하면 학습데이터의 크기가 매우 커지지만, 우리가 원하는 정보는 그리 많지 않기 때문입니다.\n",
    "\n",
    "`encode` 함수에서 문장의 앞, 뒤에 window 만큼의 unknown vocab 을 추가합니다. 이는 context words 에 대한 padding 입니다. 이후 context_idxs 에서 단순히 list slicing 만 하여도 같은 크기의 input vector 를 만들 수 있습니다.\n",
    "\n",
    "```python\n",
    "    def encode(sent):\n",
    "        idxs = [vocab_to_idx.get(w, n_vocabs) for w in sent]        \n",
    "        idxs = [n_vocabs] * window + idxs + [n_vocabs] * window\n",
    "        return idxs\n",
    "\n",
    "    word_idxs = encode(sent)\n",
    "\n",
    "    for i, word in enumerate(sent):\n",
    "        # ...\n",
    "        context_idxs = word_idxs[b:i+window] + word_idxs[i+window+1:e]\n",
    "        context = np.hstack([wv_[idx] for idx in context_idxs])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(vocab_to_idx, sents, seed_words, wv_, window=2, test_data=False):\n",
    "\n",
    "    n_vocabs = len(vocab_to_idx)\n",
    "\n",
    "    def contain_seed(sent):\n",
    "        for word in sent:\n",
    "            if word in seed_words:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def encode(sent):\n",
    "        idxs = [vocab_to_idx.get(w, n_vocabs) for w in sent]\n",
    "        # padding\n",
    "        idxs = [n_vocabs] * window + idxs + [n_vocabs] * window\n",
    "        return idxs\n",
    "\n",
    "    X = []\n",
    "    words = []\n",
    "\n",
    "    for sent in sents:\n",
    "\n",
    "        if (not test_data) and (not contain_seed(sent)):\n",
    "            continue\n",
    "\n",
    "        n_words = len(sent)\n",
    "        word_idxs = encode(sent)\n",
    "\n",
    "        for i, word in enumerate(sent):\n",
    "            if not (word in vocab_to_idx):\n",
    "                continue\n",
    "\n",
    "            b = i # i - window + window\n",
    "            e = i + 2 * window + 1 # i + window + 1 + window\n",
    "\n",
    "            context_idxs = word_idxs[b:i+window] + word_idxs[i+window+1:e]\n",
    "            context = np.hstack([wv_[idx] for idx in context_idxs])\n",
    "            X.append(context)\n",
    "            words.append(word)\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    Y = np.asarray([1 if w in seed_words else 0 for w in words], dtype=np.int)\n",
    "    return X, Y, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "184,865 개의 학습 데이터가 만들어졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((184865, 400), (184865,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, words = create_dataset(vocab_to_idx, sents, seed_words, wv_)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn 도 partial_fit 함수를 이용하면 minibatch style 로 구현할 수 있습니다. fit 함수는 모델을 처음 학습할 때 이용하며, partial_fit 은 한 번 학습된 모델을 추가로 학습할 때 이용합니다. 또한 아래처럼 이전에 만든 모델을 입력할 수 있도록 구현하면 이용하던 모델에 추가 학습도 가능합니다. \n",
    "\n",
    "Classifier 를 만들 때 `max_iter=1` 로 설정하면 minibatch 처럼 만들 수 있습니다. Loss 는 positive class 의 데이터는 negative class 의 확률, negative class 의 데이터는 positive class 의 확률입니다. 이들을 모두 더하여 epoch 마다 출력도 합니다.\n",
    "\n",
    "```python\n",
    "def minibatch_style(model=None, ... ):\n",
    "\n",
    "    if model is None:\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_size, activation='relu', max_iter=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch = 1 / 20, batch = 5 / 5, loss = 0.030123628954597047\n",
      "train epoch = 2 / 20, batch = 5 / 5, loss = 0.024039307789820643\n",
      "train epoch = 3 / 20, batch = 5 / 5, loss = 0.022244276674744426\n",
      "train epoch = 4 / 20, batch = 5 / 5, loss = 0.018842596180440145\n",
      "train epoch = 5 / 20, batch = 5 / 5, loss = 0.017784514394523864\n",
      "train epoch = 6 / 20, batch = 5 / 5, loss = 0.015585111645000937\n",
      "train epoch = 7 / 20, batch = 5 / 5, loss = 0.014611616061161743\n",
      "train epoch = 8 / 20, batch = 5 / 5, loss = 0.014042305279238498\n",
      "train epoch = 9 / 20, batch = 5 / 5, loss = 0.012519840851847274\n",
      "train epoch = 10 / 20, batch = 5 / 5, loss = 0.011991350871155422\n",
      "train epoch = 11 / 20, batch = 5 / 5, loss = 0.011581874046409077\n",
      "train epoch = 12 / 20, batch = 5 / 5, loss = 0.010860279391383175\n",
      "train epoch = 13 / 20, batch = 5 / 5, loss = 0.009968705727710523\n",
      "train epoch = 14 / 20, batch = 5 / 5, loss = 0.009508829779523905\n",
      "train epoch = 15 / 20, batch = 5 / 5, loss = 0.009257455911220751\n",
      "train epoch = 16 / 20, batch = 5 / 5, loss = 0.009077448758193257\n",
      "train epoch = 17 / 20, batch = 5 / 5, loss = 0.008480615280963852\n",
      "train epoch = 18 / 20, batch = 5 / 5, loss = 0.007712492386587986\n",
      "train epoch = 19 / 20, batch = 5 / 5, loss = 0.007850206049894028\n",
      "train epoch = 20 / 20, batch = 5 / 5, loss = 0.007002108013128572\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def minibatch_style(vocab_to_idx, sents, seed_words, wv_, model=None,\n",
    "    n_batch_sents=50000, hidden_size=(50,), epochs=20, verbose=True):\n",
    "\n",
    "    n_sents = len(sents)\n",
    "    n_batchs = math.ceil(n_sents / n_batch_sents)\n",
    "\n",
    "    if model is None:\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_size, activation='relu', max_iter=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = 0\n",
    "        n_instances = 0\n",
    "\n",
    "        for batch in range(n_batchs):\n",
    "\n",
    "            b = batch * n_batch_sents\n",
    "            e = min((batch + 1) * n_batch_sents, n_sents)\n",
    "            X, Y, words = create_dataset(vocab_to_idx, sents[b:e], seed_words, wv_)\n",
    "\n",
    "            partial_fit = (epoch > 0) or (batch > 0)\n",
    "            if partial_fit:\n",
    "                model.partial_fit(X, Y)\n",
    "            else:\n",
    "                model.fit(X, Y)\n",
    "\n",
    "            prob = model.predict_proba(X)\n",
    "            loss += prob[np.where(Y == 1)[0], 0].sum()\n",
    "            loss += prob[np.where(Y == 0)[0], 1].sum()\n",
    "            n_instances += X.shape[0]\n",
    "\n",
    "            if verbose:\n",
    "                avg_loss = loss / n_instances\n",
    "                print('\\rtrain epoch = {} / {}, batch = {} / {}, loss = {}'.format(\n",
    "                    epoch+1, epochs, batch+1, n_batchs, avg_loss), end='')\n",
    "        if verbose:\n",
    "            print()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = minibatch_style(vocab_to_idx, sents, seed_words, wv_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 `day7_2` 의 튜토리얼처럼 minibatch 형식으로 prediction 을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch prediction 5 / 5\n"
     ]
    }
   ],
   "source": [
    "def minibatch_predict(vocab_to_idx, sents, seed_words, wv_, model, n_batch_sents=50000):\n",
    "    y_prob = []\n",
    "    y_words = []\n",
    "    n_sents = len(sents)\n",
    "    n_batchs = math.ceil(n_sents / n_batch_sents)\n",
    "\n",
    "    for batch in range(n_batchs):\n",
    "\n",
    "        b = batch * n_batch_sents\n",
    "        e = min((batch + 1) * n_batch_sents, n_sents)\n",
    "        X, _, words = create_dataset(vocab_to_idx, sents[b:e], seed_words, wv_, test_data=True)\n",
    "\n",
    "        y_prob.append(model.predict_proba(X))\n",
    "        y_words += words\n",
    "\n",
    "        print('\\rbatch prediction {} / {}'.format(batch+1, n_batchs), end='')\n",
    "    print('\\rbatch prediction {0} / {0}'.format(n_batchs))\n",
    "\n",
    "    y_prob = np.vstack(y_prob)[:,1]\n",
    "    y_words=  np.asarray(y_words)\n",
    "    return y_prob, y_words\n",
    "\n",
    "politician_prob, politician_words = minibatch_predict(vocab_to_idx, sents, seed_words, wv_, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5791279,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politician_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract(y_prob, y_words, min_prob=0.6):\n",
    "    # word count\n",
    "    word_counter = Counter(y_words)\n",
    "\n",
    "    # prediction count\n",
    "    pred_pos = defaultdict(int)\n",
    "    for row in np.where(y_prob >= min_prob)[0]:\n",
    "        pred_pos[y_words[row]] += 1\n",
    "    pred_pos = {word:(word_counter[word], pos/word_counter[word])\n",
    "                for word, pos in pred_pos.items()}\n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정치인 혹은 직함의 이름을 추출하였습니다 (구글링 해보면 많은 경우 정치인임을 확인하실 수 있습니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최문영 (130)\t1.000\n",
      "구윤성 (34)\t1.000\n",
      "민경석 (36)\t1.000\n",
      "류효림 (74)\t0.986\n",
      "박태훈 (45)\t0.978\n",
      "김주형 (41)\t0.976\n",
      "이재희 (38)\t0.974\n",
      "윤동진 (42)\t0.952\n",
      "이정국 (46)\t0.913\n",
      "추연화 (150)\t0.847\n",
      "윤창원 (56)\t0.839\n",
      "임세영 (62)\t0.839\n",
      "김영석 (66)\t0.818\n",
      "최현규 (43)\t0.814\n",
      "한윤종 (51)\t0.804\n",
      "한혁승 (180)\t0.794\n",
      "허경 (130)\t0.792\n",
      "이한형 (42)\t0.786\n",
      "손진아 (40)\t0.775\n",
      "박홍규 (33)\t0.758\n",
      "신웅수 (32)\t0.750\n",
      "이지숙 (56)\t0.732\n",
      "손형주 (89)\t0.730\n",
      "0030 (33)\t0.727\n",
      "김재창 (47)\t0.723\n",
      "김나라 (42)\t0.714\n",
      "이기범 (90)\t0.711\n",
      "이승길 (37)\t0.703\n",
      "김경민 (85)\t0.694\n",
      "권현진 (111)\t0.694\n",
      "김성진 (118)\t0.644\n",
      "조성진 (53)\t0.642\n",
      "이우인 (50)\t0.640\n",
      "김풀잎 (40)\t0.625\n",
      "김수정 (77)\t0.623\n",
      "신소원 (38)\t0.605\n",
      "박현민 (30)\t0.600\n",
      "올랑드 (53)\t0.585\n",
      "곽영래 (135)\t0.578\n",
      "김주성 (48)\t0.562\n",
      "신나라 (41)\t0.561\n",
      "권현수 (36)\t0.556\n",
      "김아름 (69)\t0.551\n",
      "박귀임 (34)\t0.529\n",
      "박세연 (36)\t0.528\n",
      "이보라 (37)\t0.514\n",
      "유엄식 (100)\t0.500\n",
      "이지영 (42)\t0.500\n",
      "박세완 (276)\t0.500\n",
      "임철영 (74)\t0.500\n",
      "조호윤 (42)\t0.500\n",
      "장아름 (50)\t0.500\n",
      "한경닷컴 (329)\t0.489\n",
      "박정선 (37)\t0.486\n",
      "박지혜 (101)\t0.485\n",
      "이지현 (50)\t0.480\n",
      "김현태 (70)\t0.471\n",
      "김민영 (34)\t0.471\n",
      "노해섭 (149)\t0.470\n",
      "김미화 (49)\t0.469\n",
      "장의 (186)\t0.468\n",
      "이승현 (116)\t0.466\n",
      "박소영 (43)\t0.465\n",
      "남친 (39)\t0.462\n",
      "이해인 (141)\t0.461\n",
      "윤동주 (135)\t0.452\n",
      "홍봉진 (53)\t0.434\n",
      "김민규 (72)\t0.431\n"
     ]
    }
   ],
   "source": [
    "pred_pos = extract(politician_prob, politician_words, min_prob=0.6)\n",
    "\n",
    "for word, (count, prob) in sorted(pred_pos.items(), key=lambda x:-x[1][1])[:300]:\n",
    "    if word in seed_words or len(word) == 1:\n",
    "        continue\n",
    "    if count < 30:\n",
    "        continue\n",
    "    print('{} ({})\\t{:.3f}'.format(word, count, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 에서의 유사어 중 일부를 학습에 이용하지 않았었습니다. 이들 중 어떤 단어가 정치인으로 판단되었는지 확인도 해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전두환 (14)\t0.214\n",
      "충북도의회 (19)\t0.053\n",
      "테메르 (21)\t0.333\n",
      "통일부장관 (22)\t0.045\n",
      "호세프 (31)\t0.194\n"
     ]
    }
   ],
   "source": [
    "for query in '인수위원회 임종성 전두환 전북도지사 정우택 지우마 진선미 최연혜 충북경제자유구역청 충북도의회 테메르 통일부장관 호세프 황광 황주홍 후세인'.split():\n",
    "    if not query in pred_pos:\n",
    "        continue\n",
    "    count, prob = pred_pos[query]\n",
    "    print('{} ({})\\t{:.3f}'.format(query, count, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좀 더 저에게 익숙한 엔터테인 도메인에서 동일한 작업을 수행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda3/envs/pytorch/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "seed_words = {w for w, _ in word2vec.wv.most_similar('아이오아이', topn=20)}\n",
    "seed_words = {w for w, _ in word2vec.wv.most_similar('트와이스', topn=20)}\n",
    "seed_words.update({w for w, _ in word2vec.wv.most_similar('에이핑크', topn=20)})\n",
    "seed_words.update({w for w, _ in word2vec.wv.most_similar('강호동', topn=20)})\n",
    "seed_words = set(\n",
    "    '''강호동 경리 고복실 김국진 김규종 다이아 동방신기 듀오 레이디 몬스타엑스 바스타즈\n",
    "    박수홍 박재범 블락비 비스트 빅뱅 샤이니 세븐 손나은 신용재 신화 아이오아이 에이핑크\n",
    "    엑소 오블리스 우태운 원호 위너 이경규 이별 이화신 제작진 종이학 종현 진석 치타 컴백\n",
    "    키스신 타이틀곡 태민 트와이스 표나리 피오 형님'''.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 만들어둔 함수를 재활용 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch = 1 / 20, batch = 5 / 5, loss = 0.053350934069245924\n",
      "train epoch = 2 / 20, batch = 5 / 5, loss = 0.042205496559212744\n",
      "train epoch = 3 / 20, batch = 5 / 5, loss = 0.03708257471960816\n",
      "train epoch = 4 / 20, batch = 5 / 5, loss = 0.033432014348373035\n",
      "train epoch = 5 / 20, batch = 5 / 5, loss = 0.031528370472947724\n",
      "train epoch = 6 / 20, batch = 5 / 5, loss = 0.029480737355456765\n",
      "train epoch = 7 / 20, batch = 5 / 5, loss = 0.027266589363529582\n",
      "train epoch = 8 / 20, batch = 5 / 5, loss = 0.027260558323314196\n",
      "train epoch = 9 / 20, batch = 5 / 5, loss = 0.025027976586050694\n",
      "train epoch = 10 / 20, batch = 5 / 5, loss = 0.024923751212790484\n",
      "train epoch = 11 / 20, batch = 5 / 5, loss = 0.023443946192445084\n",
      "train epoch = 12 / 20, batch = 5 / 5, loss = 0.023201704083418838\n",
      "train epoch = 13 / 20, batch = 5 / 5, loss = 0.021967246275630063\n",
      "train epoch = 14 / 20, batch = 5 / 5, loss = 0.022427403381689545\n",
      "train epoch = 15 / 20, batch = 5 / 5, loss = 0.020804412590573025\n",
      "train epoch = 16 / 20, batch = 5 / 5, loss = 0.020921985152361475\n",
      "train epoch = 17 / 20, batch = 5 / 5, loss = 0.019275614672091834\n",
      "train epoch = 18 / 20, batch = 5 / 5, loss = 0.019576192805807833\n",
      "train epoch = 19 / 20, batch = 5 / 5, loss = 0.018545169358491896\n",
      "train epoch = 20 / 20, batch = 5 / 5, loss = 0.017882175994366592\n",
      "batch prediction 5 / 5\n"
     ]
    }
   ],
   "source": [
    "model = minibatch_style(vocab_to_idx, sents, seed_words, wv_)\n",
    "entertainer_prob, entertainer_words = minibatch_predict(vocab_to_idx, sents, seed_words, wv_, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회사 이름이나 정치 용어가 섞여있긴 하지만 많은 단어들이 엔터테이너 혹은 엔터테인 관련 단어입니다. 그리고 이러한 window classification 기반 학습 방법은 positive class 의 단어들이 주로 등장했던 문맥을 그대로 외운 뒤, 동일한 문맥에 등장한 단어를 탐색합니다. 그렇기 때문에 토픽이 조금 다르다면 동일한 문맥이 없을 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예정화 (50)\t0.880\n",
      "티아라 (130)\t0.462\n",
      "서인영 (140)\t0.443\n",
      "화제작 (62)\t0.323\n",
      "코스피지수 (76)\t0.276\n",
      "허지웅 (87)\t0.276\n",
      "현아 (155)\t0.265\n",
      "박수진 (90)\t0.256\n",
      "한강수상택시 (57)\t0.246\n",
      "알맹 (53)\t0.245\n",
      "고주원 (54)\t0.241\n",
      "곽정은 (175)\t0.240\n",
      "씨스타 (123)\t0.236\n",
      "조희연 (56)\t0.232\n",
      "이태란 (53)\t0.226\n",
      "리더십 (160)\t0.225\n",
      "설리 (69)\t0.217\n",
      "검사장 (152)\t0.217\n",
      "호텔신라 (135)\t0.215\n",
      "삼성중공업 (80)\t0.212\n",
      "차오루 (76)\t0.211\n",
      "연방준비제도 (67)\t0.209\n",
      "소나무 (117)\t0.205\n",
      "김준수 (153)\t0.203\n",
      "김장훈 (101)\t0.198\n",
      "연구팀 (133)\t0.195\n",
      "트레 (176)\t0.193\n",
      "산업은행장 (58)\t0.190\n",
      "나비 (75)\t0.187\n",
      "김진솔 (75)\t0.187\n",
      "최규선 (61)\t0.180\n",
      "취객 (50)\t0.180\n",
      "금융노조 (50)\t0.180\n",
      "상추 (68)\t0.176\n",
      "동맹들 (51)\t0.176\n",
      "성훈 (103)\t0.175\n",
      "평년 (92)\t0.174\n",
      "홍진경 (58)\t0.172\n",
      "박경리문학상 (101)\t0.168\n",
      "소녀시대 (66)\t0.167\n",
      "이스트소프트 (79)\t0.165\n",
      "김병기 (152)\t0.164\n",
      "서문탁 (215)\t0.163\n",
      "사돈 (62)\t0.161\n",
      "지숙 (56)\t0.161\n",
      "강민혁 (51)\t0.157\n",
      "국회의원들 (64)\t0.156\n",
      "박보영 (96)\t0.156\n",
      "레인보우 (103)\t0.155\n",
      "설현 (58)\t0.155\n",
      "자료 (1387)\t0.154\n",
      "김제동 (72)\t0.153\n",
      "코스닥지수 (53)\t0.151\n",
      "선미 (67)\t0.149\n",
      "스태프들 (191)\t0.147\n",
      "도철 (89)\t0.146\n",
      "장도연 (55)\t0.145\n",
      "신한은행 (166)\t0.145\n",
      "김지민 (287)\t0.143\n",
      "진영 (303)\t0.142\n",
      "오연서 (100)\t0.140\n",
      "나인뮤지스 (165)\t0.139\n",
      "문가영 (72)\t0.139\n",
      "대원들 (52)\t0.135\n",
      "유럽우주국 (60)\t0.133\n",
      "불승인 (60)\t0.133\n",
      "옥션 (98)\t0.133\n",
      "원금 (54)\t0.130\n",
      "합참 (85)\t0.129\n",
      "조진웅 (72)\t0.125\n",
      "다현 (56)\t0.125\n",
      "박은지 (89)\t0.124\n",
      "정인영 (89)\t0.124\n",
      "셀트리온 (65)\t0.123\n",
      "을지대병원 (57)\t0.123\n",
      "효과적 (327)\t0.122\n",
      "부산은행 (140)\t0.121\n",
      "경기도교육청 (66)\t0.121\n",
      "경찰 (5173)\t0.120\n",
      "경비원 (126)\t0.119\n",
      "스타제국 (93)\t0.118\n",
      "제임스 (127)\t0.118\n",
      "솔비 (51)\t0.118\n",
      "김종민 (94)\t0.117\n",
      "손호영 (180)\t0.117\n",
      "원심 (69)\t0.116\n",
      "산전 (156)\t0.115\n",
      "나다 (78)\t0.115\n",
      "김명민 (87)\t0.115\n",
      "이성호 (87)\t0.115\n",
      "전경 (149)\t0.114\n",
      "서장 (53)\t0.113\n",
      "코스피 (249)\t0.112\n",
      "빅스 (179)\t0.112\n",
      "경찰관들 (171)\t0.111\n",
      "손예진 (162)\t0.111\n",
      "중앙응급의료위원회 (55)\t0.109\n",
      "기안84 (138)\t0.109\n",
      "김가연 (185)\t0.108\n",
      "아우디폭스바겐코리아 (139)\t0.108\n",
      "복실 (427)\t0.108\n",
      "유재석 (65)\t0.108\n",
      "배두나 (56)\t0.107\n",
      "유해진 (140)\t0.107\n",
      "임우재 (272)\t0.107\n",
      "금감원 (122)\t0.107\n",
      "삼성화재 (85)\t0.106\n",
      "오랜만 (173)\t0.104\n",
      "텔레콤 (402)\t0.102\n",
      "연준 (206)\t0.102\n",
      "용산 (59)\t0.102\n",
      "관람객들 (118)\t0.102\n",
      "이웃들 (59)\t0.102\n",
      "동부대우전자 (69)\t0.101\n",
      "현대건설 (69)\t0.101\n",
      "경기도지사 (89)\t0.101\n",
      "부산경찰청 (70)\t0.100\n",
      "금융지주 (50)\t0.100\n",
      "유럽중앙은행 (90)\t0.100\n",
      "감사원 (70)\t0.100\n",
      "문준영 (50)\t0.100\n",
      "남성들 (121)\t0.099\n",
      "국립과학수사연구원 (91)\t0.099\n",
      "오윤아 (61)\t0.098\n",
      "롯데홀딩스 (51)\t0.098\n",
      "한국전력 (92)\t0.098\n",
      "김소현 (72)\t0.097\n",
      "서울시장 (124)\t0.097\n",
      "병무청 (62)\t0.097\n",
      "나리 (248)\t0.097\n",
      "비주류 (52)\t0.096\n",
      "유시민 (52)\t0.096\n",
      "관광객들 (189)\t0.095\n",
      "여자들 (63)\t0.095\n",
      "공정거래위원회 (95)\t0.095\n",
      "제아 (53)\t0.094\n",
      "리얼미터 (96)\t0.094\n",
      "10회 (64)\t0.094\n",
      "야놀자 (54)\t0.093\n",
      "김효진 (54)\t0.093\n",
      "베이지북 (175)\t0.091\n",
      "기상청 (274)\t0.091\n",
      "발족 (66)\t0.091\n",
      "최고위원 (55)\t0.091\n",
      "윤도현 (77)\t0.091\n",
      "경장 (56)\t0.089\n",
      "건설사들 (56)\t0.089\n",
      "산림청 (91)\t0.088\n",
      "합동참모본부 (81)\t0.086\n",
      "검찰 (2400)\t0.086\n",
      "연구진 (151)\t0.086\n",
      "전남대병원 (105)\t0.086\n",
      "타인 (70)\t0.086\n",
      "조재현 (70)\t0.086\n",
      "수지 (135)\t0.081\n",
      "육군 (86)\t0.081\n",
      "대전시 (74)\t0.081\n",
      "이철성 (136)\t0.081\n",
      "환자들 (112)\t0.080\n",
      "엄마들 (112)\t0.080\n",
      "러에코 (50)\t0.080\n",
      "김수현 (100)\t0.080\n",
      "최대주주 (163)\t0.080\n",
      "질병관리본부 (63)\t0.079\n",
      "중소기업중앙회 (63)\t0.079\n",
      "도민 (63)\t0.079\n",
      "안개 (177)\t0.079\n",
      "강타 (127)\t0.079\n",
      "황보 (51)\t0.078\n",
      "메르세데스 (64)\t0.078\n",
      "피에스타 (64)\t0.078\n",
      "대원 (78)\t0.077\n",
      "중환자실 (52)\t0.077\n"
     ]
    }
   ],
   "source": [
    "pred_pos = extract(entertainer_prob, entertainer_words, min_prob=0.85)\n",
    "\n",
    "for word, (count, prob) in sorted(pred_pos.items(), key=lambda x:-x[1][1])[:1000]:\n",
    "    if word in seed_words or len(word) == 1:\n",
    "        continue\n",
    "    if count < 50:\n",
    "        continue\n",
    "    print('{} ({})\\t{:.3f}'.format(word, count, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
