{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA 의 (doc, topic) 확률 벡터 calibration 과 perplexity 계산\n",
    "\n",
    "Gensim 의 LDA 는 각 문서의 토픽 확률 벡터를 추정할 때, 각 토픽에 기본 확률을 부여합니다. 이를 조정하여 sparse 한 토픽 벡터가 되도록 하는 후처리 과정입니다.\n",
    "\n",
    "Bag of words model 인 `x` 와 미리 학습한 LDA 모델을 로딩합니다. 30,091 개의 문서에서 100 개의 토픽이 학습되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.492\n",
      "added lovit_textmining_dataset\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.matutils import dirichlet_expectation\n",
    "from lovit_textmining_dataset.navernews_10days import get_bow\n",
    "\n",
    "\n",
    "x, idx_to_vocab, vocab_to_idx = get_bow(date='2016-10-20', tokenize='noun')\n",
    "corpus = gensim.matutils.Sparse2Corpus(x, documents_columns=False)\n",
    "id2word = dict(enumerate(idx_to_vocab))\n",
    "\n",
    "ldamodel_fname = './2016-10-20-lda.pkl'\n",
    "with open(ldamodel_fname, 'rb') as f:\n",
    "    ldamodel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity 계산\n",
    "\n",
    "Perplexity 는 아래의 식을 구현하였습니다.\n",
    "\n",
    "$\\log { \\left\\{ p(w) \\right\\}  } =\\sum _{ d=1 }^{ D }{ \\sum _{ j=1 }^{ V }{ { n }^{ jd }\\log { \\left[ \\sum _{ k=1 }^{ K }{ { \\theta  }_{ k }^{ d }{ \\phi  }_{ k }^{ j } }  \\right]  }  }  }$\n",
    "\n",
    "$Perplexity(w)=exp\\left[ -\\frac { log\\left\\{ p(w) \\right\\}  }{ \\sum _{ d=1 }^{ D }{ \\sum _{ j=1 }^{ V }{ { n }^{ jd } }  }  }  \\right]$\n",
    "\n",
    "단, 그 과정에서 (문서, 토픽) 확률과 (토픽, 단어) 확률의 곱에 의해 (문서, 단어) 크기의 numpy.ndarray 가 한 번 계산되는데, 이 크기의 dense matrix 가 한 번에 만들려면 많은 메모리를 이용해야 합니다. 이를 방지하기 위하여 `chunk` 의 개수만큼의 문서만 잘라서 이 과정을 반복하도록 구현하였습니다.\n",
    "\n",
    "또한 (문서, 단어) 벡터 행렬의 값을 로그로 변환하는 과정에서 log (0) -inf 로 계산되는 것을 방지하기 위하여 그 곱이 0 인 값은 1 로 변환하는 부분을 추가하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def compute_perplexity(bow, topic_term_prob, doc_topic_prob, chunk=1000):\n",
    "    n_docs = bow.shape[0]\n",
    "    log_perp = 0\n",
    "    for c in range(math.ceil(n_docs / chunk)):\n",
    "        b = int(c * chunk)\n",
    "        e = min(n_docs, int((c+1) * chunk))\n",
    "        doc_term_prod = doc_topic_prob[b:e].dot(topic_term_prob)\n",
    "        doc_term_prod[np.where(doc_term_prod == 0)] = 1\n",
    "        doc_term_prod = -1 * np.log(doc_term_prod)\n",
    "        doc_term_prod = np.nan_to_num(doc_term_prod)\n",
    "        log_perp += bow[b:e].multiply(doc_term_prod).sum()\n",
    "    log_perp /= bow.sum()\n",
    "    return log_perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 학습된 Gensim LDA 에서 (문서, 토픽) 확률 행렬과 (토픽, 단어) 확률 행렬을 가져오는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(lda, corpus):\n",
    "    topic_term_prob = _get_topic_term_prob(lda)\n",
    "    doc_topic_freq = _get_doc_topic_freq(lda, corpus)\n",
    "\n",
    "    doc_topic_prob = doc_topic_freq / doc_topic_freq.sum(axis=1)[:, None]\n",
    "    topic_prob = doc_topic_freq.sum(axis=0) / doc_topic_freq.sum()\n",
    "\n",
    "    return topic_term_prob, doc_topic_prob, topic_prob\n",
    "\n",
    "def _get_topic_term_prob(lda):\n",
    "    topic_term_freq = lda.state.get_lambda()\n",
    "    topic_term_prob = topic_term_freq / topic_term_freq.sum(axis=1)[:, None]\n",
    "    return topic_term_prob\n",
    "\n",
    "def _get_doc_topic_freq(lda, corpus):\n",
    "    doc_topic_freq, _ = lda.inference(corpus)\n",
    "    return doc_topic_freq\n",
    "\n",
    "topic_term_prob, doc_topic_prob, topic_prob = get_parameters(ldamodel, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 LDA 모델에 대한 perplexity 를 계산합니다. 아래의 값은 log 를 씌운 값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.36096462712405"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(x, topic_term_prob, doc_topic_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 1 번 문서의 토픽 벡터를 살펴보면 대부분의 토픽에 0.2 % 의 확률이 부여되어 있습니다. 사실 이 문서를 표현하는 토픽은 0.2 보다 큰 세 개의 토픽인데, 0.2 % 의 97 개 토픽에 의하여 약 20 % 의 확률이 쓰이질 못하고 있습니다. 이 값을 0 으로 변환하여 sparse 한 토픽 벡터를 만들어 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.27117783, 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.20199998, 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.3328221 , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ,\n",
       "       0.002     , 0.002     , 0.002     , 0.002     , 0.002     ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_prob[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calibrate_doc_topic_prob` 함수는 각 문서의 토픽 벡터에서 최소값을 찾은 뒤, 벡터에서 이 값을 빼고 벡터의 L1 norm 이 1 이 되도록 조절하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def calibrate_doc_topic_prob(doc_topic_prob, min_prob=None, uniform_as_zero=False):\n",
    "    n_docs, n_topics = doc_topic_prob.shape\n",
    "    if min_prob is None:\n",
    "        min_prob = doc_topic_prob.min(axis=1)\n",
    "    if n_docs != min_prob.shape[0]:\n",
    "        raise ValueError('min_prob length must be same with num of docs')\n",
    "    shift = np.repeat(min_prob, n_topics, axis=0).reshape(-1, n_topics)\n",
    "    calibrated_doc_topic_prob = doc_topic_prob - shift\n",
    "    calibrated_doc_topic_prob = normalize(calibrated_doc_topic_prob, norm='l1')\n",
    "    if uniform_as_zero:\n",
    "        calibrated_doc_topic_prob = probablity_normalization(calibrated_doc_topic_prob)\n",
    "    return calibrated_doc_topic_prob\n",
    "\n",
    "def probablity_normalization(prob):\n",
    "    row_sum = prob.sum(axis=1)\n",
    "    base = 1 / prob.shape[1]\n",
    "    prob[np.where(row_sum == 0)[0]] = base\n",
    "    return prob\n",
    "\n",
    "calibrated_doc_topic_prob = calibrate_doc_topic_prob(doc_topic_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 적용 결과 세 개의 토픽의 확률값은 이전보다 커졌고, 나머지 97 개의 토픽의 확률값은 0 이 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.33647233, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.25      , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4135277 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibrated_doc_topic_prob[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 log perplexity 는 7.36096462712405 이었는데, 그 때보다는 log perplexity 가 조금 줄어들었음을 알 수 있습니다. 이는 각 확률에 부여되는 기본값에 의하여 LDA 의 설명력이 약해졌다는 의미입니다. Sparse topic vector 로 변환함으로써 조금 더 설명력이 좋은 LDA 모델이 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.355193965112524"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(x, topic_term_prob, calibrated_doc_topic_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging topic\n",
    "\n",
    "때로는 후처리 과정에서 몇 개의 토픽을 하나로 병합하고 싶기도 합니다. `merge_topic` 함수는 이를 위한 함수로, (토픽, 단어) 행렬, (문서, 토픽) 행렬, 문서의 길이 벡터, 그리고 병합할 토픽의 아이디를 입력하면 이를 병합하는 함수입니다. 병합되지 않는 토픽을 맨 앞에, 병합하는 토픽이 마지막의 아이디를 가지도록 만들었습니다.\n",
    "\n",
    "각 문서마다 길이가 다르기 때문에 토픽을 병합할 때는 토픽 별 가중치가 달라야 합니다. (문서, 토픽) 확률 행렬과 문서의 길이 벡터를 이용하여 (문서, 토픽) 빈도 행렬을 만든 뒤, 각 토픽의 확률을 계산합니다. 이 값을 가중치로 이용하여 토픽을 병합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def merge_topic(topic_term_prob, doc_topic_prob, doc_lengths, merge_topic_idxs):\n",
    "    pos_idx = np.asarray(merge_topic_idxs)\n",
    "    neg_idx = np.asarray(\n",
    "        [i for i in range(topic_term_prob.shape[0])\n",
    "         if not (i in merge_topic_idxs)]\n",
    "    )\n",
    "\n",
    "    doc_topic_freq = np.diag(doc_lengths).dot(doc_topic_prob)\n",
    "    topic_freq = np.asarray(doc_topic_freq.sum(axis=0)).reshape(-1)\n",
    "    topic_term_freq = np.diag(topic_freq).dot(topic_term_prob)\n",
    "\n",
    "    doc_topic_freq_pos = np.asarray(doc_topic_freq[:,pos_idx].sum(axis=1)).reshape(-1,1)\n",
    "    doc_topic_freq_neg = doc_topic_freq[:,neg_idx]\n",
    "    doc_topic_prob_ = np.hstack([doc_topic_freq_pos, doc_topic_freq_neg])\n",
    "    topic_prob_ = normalize(np.asarray(doc_topic_prob_.sum(axis=0)).reshape(1,-1)).reshape(-1)\n",
    "    doc_topic_prob_ = normalize(doc_topic_prob_, norm='l1')\n",
    "\n",
    "    topic_term_pos = np.asarray(topic_term_freq[pos_idx].sum(axis=0)).reshape(1,-1)\n",
    "    topic_term_neg = topic_term_freq[neg_idx]\n",
    "    topic_term_prob_ = normalize(np.vstack([topic_term_pos, topic_term_neg]), norm='l1')\n",
    "\n",
    "    return doc_topic_prob_, topic_term_prob_, topic_prob_\n",
    "\n",
    "def sum_bow(bow):\n",
    "    doc_lengths = np.asarray(bow.sum(axis=1)).reshape(-1)\n",
    "    term_frequency = np.asarray(bow.sum(axis=0)).reshape(-1)\n",
    "    return doc_lengths, term_frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의로 1, 3, 4 번의 토픽을 하나로 병합하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths, term_frequency = sum_bow(x)\n",
    "doc_topic_prob_, topic_term_prob_, topic_prob_ = merge_topic(topic_term_prob, doc_topic_prob, doc_lengths, [1,3,4])\n",
    "\n",
    "doc_topic_prob_ = probablity_normalization(doc_topic_prob_)\n",
    "topic_term_prob_ = probablity_normalization(topic_term_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 결과 100 개에서 98 개로 토픽의 개수가 줄어든 (문서, 토픽) 행렬이 만들어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30091, 98)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_prob_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 역시 LDAvis 를 이용하여 시각화 할 수 있습니다. 결과를 살펴보면 토픽의 개수가 98 개로 줄어들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda3/envs/pytorch/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "from pyLDAvis import prepare, show, save_html\n",
    "\n",
    "prepared_data = prepare(\n",
    "    topic_term_prob_,\n",
    "    doc_topic_prob_,\n",
    "    doc_lengths,\n",
    "    idx_to_vocab,\n",
    "    term_frequency,\n",
    "    mds = 'tsne',\n",
    "    plot_opts = {'xlab': 't-SNE1', 'ylab': 't-SNE2'},\n",
    "    R = 30 # num of displayed terms\n",
    ")\n",
    "\n",
    "save_html(prepared_data, './2016-10-20-pyldavis_t98.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
