{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word adjacent graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.492\n",
      "added lovit_textmining_dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['성씨/NNP',\n",
       " '강북/NNP',\n",
       " '인근/NNG',\n",
       " '치킨/NNP',\n",
       " '집/NNG',\n",
       " '이씨/NNP',\n",
       " '뒤/NNG',\n",
       " '쫓/VV',\n",
       " '실랑이/NNG',\n",
       " '쓰러뜨리/VV',\n",
       " '후/NNG',\n",
       " '총기/NNG',\n",
       " '가져오/VV',\n",
       " '망치/NNP',\n",
       " '이씨/NNP',\n",
       " '머리/NNG',\n",
       " '때리/VV']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "from navernews_10days import get_news_paths\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "path = get_news_paths(date='2016-10-20', tokenize='komoran')\n",
    "sents = DoublespaceLineCorpus(path, iter_sent=True, num_sent=10000)\n",
    "\n",
    "def tokenizer(sent):\n",
    "    words = sent.split()\n",
    "    words = [w for w in words if ('/N' in w) or ('/V' in w)]\n",
    "    return words\n",
    "\n",
    "sent = '성씨/NNP 는/JX 강북/NNP 서/JKB 인근/NNG 치킨/NNP 집/NNG 까지/JX 이씨/NNP 뒤/NNG 를/JKO 쫓/VV 으며/EC 실랑이/NNG 하/XSV 다/EC 쓰러뜨리/VV ㄴ/ETM 후/NNG 총기/NNG 와/JC 함께/MAG 가져오/VV ㄴ/ETM 망치/NNP 로/JKB 이씨/NNP 머리/NNG 를/JKO 때리/VV 었/EP 다/EC'\n",
    "tokenizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct adjacent graph was done                    \n",
      "transforming dict to sparse was done                    \n",
      "(3430, 3430)\n",
      "(18816,)\n"
     ]
    }
   ],
   "source": [
    "from graphutils import sents_to_adjacent_graph\n",
    "\n",
    "X, idx_to_vocab = sents_to_adjacent_graph(sents, tokenizer,\n",
    "    min_count=10, min_cooccurrence=3, verbose=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(X.nonzero()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct adjacent graph was done                    \n",
      "transforming dict to sparse was done                    \n",
      "(3430, 3430)\n",
      "(497512,)\n"
     ]
    }
   ],
   "source": [
    "X, idx_to_vocab = sents_to_adjacent_graph(sents, tokenizer,\n",
    "    min_count=10, min_cooccurrence=3, window=-1, verbose=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(X.nonzero()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank\n",
    "\n",
    "PageRank 의 구현체입니다. matrix multiplication 으로 구현하면 빠른 속도로 계산이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def pagerank(inbound_matrix, df=0.85, max_iter=30):\n",
    "    n_nodes, initial_weight, rank, bias = initialize_rank_parameters(inbound_matrix, df)\n",
    "\n",
    "    for n_iter in range(1, max_iter + 1):\n",
    "        rank_new = update_pagerank(inbound_matrix, rank, bias, df)\n",
    "        diff = np.sqrt(((rank - rank_new) **2).sum())\n",
    "        rank = rank_new\n",
    "        print('iter {}, diff = {}'.format(n_iter, diff))\n",
    "\n",
    "    return rank\n",
    "\n",
    "def initialize_rank_parameters(inbound_matrix, df):\n",
    "    # Check number of nodes and initial weight\n",
    "    n_nodes = inbound_matrix.shape[0]\n",
    "    initial_weight = 1 / n_nodes\n",
    "\n",
    "    # Initialize rank and bias\n",
    "    rank = np.asarray([initial_weight] * n_nodes)\n",
    "    bias = rank.copy()\n",
    "    return n_nodes, initial_weight, rank, bias\n",
    "\n",
    "def update_pagerank(inbound_matrix, rank, bias, df):\n",
    "    # call scipy.sparse safe_sparse_dot()\n",
    "    rank_new = inbound_matrix.dot(rank)\n",
    "    rank_new = normalize(rank_new.reshape(1, -1), norm='l2').reshape(-1)\n",
    "    rank_new = df * rank_new + (1 - df) * bias\n",
    "    return rank_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1, diff = 0.8461472508410279\n",
      "iter 2, diff = 0.0777242695496277\n",
      "iter 3, diff = 0.007689184017572718\n",
      "iter 4, diff = 0.0018597117256751708\n",
      "iter 5, diff = 0.0007008521153693605\n",
      "iter 6, diff = 0.00028895253132882245\n",
      "iter 7, diff = 0.0001208755391157564\n",
      "iter 8, diff = 5.068918444458727e-05\n",
      "iter 9, diff = 2.1265807713830784e-05\n",
      "iter 10, diff = 8.922442081999404e-06\n",
      "iter 11, diff = 3.743625331167742e-06\n",
      "iter 12, diff = 1.5707329574645392e-06\n",
      "iter 13, diff = 6.590412604139278e-07\n",
      "iter 14, diff = 2.7651768319780554e-07\n",
      "iter 15, diff = 1.160200964195273e-07\n",
      "iter 16, diff = 4.867921227165706e-08\n",
      "iter 17, diff = 2.0424614080422035e-08\n",
      "iter 18, diff = 8.569671135601644e-09\n",
      "iter 19, diff = 3.5956262515624685e-09\n",
      "iter 20, diff = 1.5086368010609492e-09\n",
      "iter 21, diff = 6.329886936877739e-10\n",
      "iter 22, diff = 2.6558579463019024e-10\n",
      "iter 23, diff = 1.1143336226411139e-10\n",
      "iter 24, diff = 4.675530644067518e-11\n",
      "iter 25, diff = 1.9616589718925564e-11\n",
      "iter 26, diff = 8.23153875546137e-12\n",
      "iter 27, diff = 3.4535199898209234e-12\n",
      "iter 28, diff = 1.4499509246521451e-12\n",
      "iter 29, diff = 6.077630155222096e-13\n",
      "iter 30, diff = 2.552724135619294e-13\n"
     ]
    }
   ],
   "source": [
    "pr = pagerank(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soygraph 의 PageRank 에 위의 코드를 정리해 두었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 : diff = 0.8461472508410279 (0.002 sec)\n",
      "iter 2 : diff = 0.0777242695496277 (0.002 sec)\n",
      "iter 3 : diff = 0.007689184017572718 (0.001 sec)\n",
      "iter 4 : diff = 0.0018597117256751708 (0.001 sec)\n",
      "iter 5 : diff = 0.0007008521153693605 (0.001 sec)\n",
      "iter 6 : diff = 0.00028895253132882245 (0.001 sec)\n",
      "iter 7 : diff = 0.0001208755391157564 (0.001 sec)\n",
      "Early stop. because it already converged.\n"
     ]
    }
   ],
   "source": [
    "from soygraph.ranking import PageRank\n",
    "\n",
    "pr = PageRank().rank(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank\n",
    "\n",
    "Rank 가 높은 단어들을 선택하면 `서울`, `연합뉴스`와 같은 일반 명사들이 키워드로 선택됩니다. 이유는 문서 집합이 여러 주제가 섞여 있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| 이/VCP  | 있/VV   | 일/NNB  | 연합뉴스/NNP | 것/NNB  | \n",
      "| 하/VV   | 서울/NNP | 기자/NNG | 등/NNB  | 이/NP   | \n",
      "| 수/NNB  | 열리/VV  | 오전/NNP | 말/NNG  | 대하/VV  | \n",
      "| 년/NNB  | 위하/VV  | 되/VV   | 미국/NNP | 북한/NNP | \n",
      "| 않/VX   | 받/VV   | 미/NNP  | 밝히/VV  | 없/VA   | \n",
      "| 천/NR   | 명/NNB  | 관련/NNG | 원/NNB  | 만/NR   | \n",
      "| 가/VV   | 국회/NNG | 대표/NNG | 장관/NNG | 제공/NNG | \n",
      "| 씨/NNB  | 한국/NNP | 오후/NNG | 문제/NNG | 지/VX   | \n",
      "| 중/NNB  | 늘/VV   | 대통령/NNG | 보/VV   | 의원/NNG | \n",
      "| 트럼프/NNP | 우리/NP  | 이날/NNG | 회의/NNG | 그/NP   | \n",
      "| 국가/NNG | 따르/VV  | 오/VV   | 시간/NNG | 억/NR   | \n",
      "| 억제/NNG | 주/VX   | 간/NNB  | 아니/VCN | 앞/NNG  | \n",
      "| 김/NNP  | 지나/VV  | 정부/NNG | 중국/NNP | 관계자/NNG | \n",
      "| 통하/VV  | 대선/NNG | 을/NNG  | 조사/NNG | 시/NNB  | \n",
      "| 클린턴/NNP | 개/NNB  | 운영/NNG | 미사일/NNG | 현지/NNG | \n",
      "| 만들/VV  | 차/NNB  | 계획/NNG | 외교/NNG | 맞/VV   | \n",
      "| 부산/NNP | 이번/NNG | 대/NNB  | 뒤/NNG  | 보이/VV  | \n",
      "| 왼쪽/NNG | 같/VA   | 확장/NNG | 크/VA   | 경찰/NNG | \n",
      "| 참석/NNG | 때문/NNB | 때/NNG  | 교육/NNG | 주장/NNG | \n",
      "| 위원회/NNP | 핵/NNG  | 발언/NNG | 사업/NNG | 수석/NNP | "
     ]
    }
   ],
   "source": [
    "top_idxs = pr.argsort()[::-1][:100]\n",
    "top_ranks = pr[top_idxs]\n",
    "\n",
    "for i, (idx, rank) in enumerate(zip(top_idxs, top_ranks)):\n",
    "    vocab = idx_to_vocab[idx]\n",
    "    if i % 5 == 0:\n",
    "        print('\\n| ', end='')\n",
    "    print('{:6}'.format(vocab, rank), end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
