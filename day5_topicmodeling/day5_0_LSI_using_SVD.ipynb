{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미리 만들어둔 document - term matrix 를 이용하여 LSI 를 학습합니다. 이를 위해서 SVD 를 직접 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.493\n",
      "added lovit_textmining_dataset\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from lovit_textmining_dataset.navernews_10days import get_bow\n",
    "\n",
    "x, idx_to_vocab, vocab_to_idx = get_bow(date='2016-10-20', tokenize='noun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서마다 단어의 개수가 다르기 때문에 L2 normalization 을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "x_ = normalize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TruncatedSVD 를 이용하면 n_components 차원으로 문서와 단어의 공간을 바꿀 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 s, sys: 15.6 s, total: 30.7 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "y = svd.fit_transform(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document - term matrix 는 (30091, 9774) 의 행렬이었습니다. 9,774 개의 단어로 이뤄진 문서의 공간이 100 차원의 공간으로 바뀌었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30091, 9774) (30091, 100)\n"
     ]
    }
   ],
   "source": [
    "print(x_.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어에 대한 100 차원의 벡터는 components_ 에 저장되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9774)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 이용하여 topically similar terms 와 topically similar docs 를 찾을 수 있습니다.\n",
    "\n",
    "Topically similar terms 를 찾는 함수를 만듭니다.\n",
    "\n",
    "이는 다른 코드에서도 재사용 할 수 있도록 word vector (wv), vocabulary list (idx_to_vocab), vocabulary encoder (vocab_to_idx) 를 arguments 로 받을 수 있도록 만듭니다. 이 함수는 utils.py 파일에 저장해둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def most_similar_terms(term, wv, idx_to_vocab, vocab_to_idx, topn=10):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    term : str\n",
    "        Query term\n",
    "    wv : numpy.ndarray or scipy.sparse.matrix\n",
    "        Word representation matrix. shape = (n_terms, dim)\n",
    "    idx_to_vocab : list of str\n",
    "        Index to word\n",
    "    vocab_to_idx : {str:int}\n",
    "        Word to index\n",
    "    topn : int\n",
    "        Number of most similar words\n",
    "    \"\"\"\n",
    "\n",
    "    # encode term as index\n",
    "    idx = vocab_to_idx.get(term, -1)\n",
    "    if idx < 0:\n",
    "        return []\n",
    "    \n",
    "    # prepare query term vector\n",
    "    query_vec = wv[idx,:].reshape(1,-1)\n",
    "\n",
    "    # compute cosine - distance\n",
    "    dist = pairwise_distances(\n",
    "        wv,\n",
    "        query_vec,\n",
    "        metric='cosine'\n",
    "    ).reshape(-1)\n",
    "\n",
    "    # find most closest terms\n",
    "    # ignore query term itself\n",
    "    similar_idx = dist.argsort()[1:topn+1]\n",
    "\n",
    "    # get their distance\n",
    "    similar_dist = dist[similar_idx]\n",
    "\n",
    "    # format : [(term, similarity), ... ]\n",
    "    similar_terms = [(idx, 1 - d) for idx, d in zip(similar_idx, similar_dist)]\n",
    "\n",
    "    # decode term index to vocabulary\n",
    "    similar_terms = [(idx_to_vocab[idx], d) for idx, d in similar_terms]\n",
    "\n",
    "    # return\n",
    "    return similar_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TruncatedSVD.components_ 는 (dim, n_terms) 의 모양입니다. transpose 를 하여 word vector 를 wv 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose (100, 9774) -> (9774, 100)\n",
    "wv = svd.components_.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'아이오아이'의 topically similar terms 입니다. 비슷한 토픽에서 등장하는 아이돌 관련 단어들이 선택됩니다. tuple 의 숫자는 cosine similarity 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('신용재', 0.9619944661010452),\n",
       " ('너무너무너무', 0.9500976581788422),\n",
       " ('오블리스', 0.9497216288897583),\n",
       " ('엠카운트다운', 0.9474410409197359),\n",
       " ('빅브레인', 0.9457024049155245),\n",
       " ('갓세븐', 0.9317425042665922),\n",
       " ('세븐', 0.9259231907701305),\n",
       " ('열창', 0.9004361633970785),\n",
       " ('산들', 0.8918696823555596),\n",
       " ('다비치', 0.8759927546317297)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_terms('아이오아이', wv, idx_to_vocab, vocab_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'오바마'의 topically similar terms 입니다. 미국 대선이 한창이던 시절의 뉴스입니다. 미국 대선 관련 단어들이 선택됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('버락', 0.9715665349517577),\n",
       " ('백악관', 0.8985053705853924),\n",
       " ('주지사', 0.8069562787276525),\n",
       " ('꼭두각시', 0.7987454049867109),\n",
       " ('공화당', 0.7858662170630752),\n",
       " ('토론장', 0.7853423244082052),\n",
       " ('월러스', 0.7821006130068013),\n",
       " ('클린턴', 0.7794028240396036),\n",
       " ('푸틴', 0.777821403814568),\n",
       " ('도널드', 0.7769946965825345)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_terms('오바마', wv, idx_to_vocab, vocab_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('클린턴', 0.984377269174501),\n",
       " ('승복', 0.9803727511690361),\n",
       " ('선거조작', 0.9764612888080014),\n",
       " ('힐러리', 0.9666305685981885),\n",
       " ('공화당', 0.9661388089142507),\n",
       " ('토론', 0.965848817571503),\n",
       " ('유권자들', 0.963856080623539),\n",
       " ('도널드', 0.9624615703856234),\n",
       " ('지저분', 0.9614910167532831),\n",
       " ('음담패설', 0.9588379161789311)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_terms('트럼프', wv, idx_to_vocab, vocab_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어와 문서의 100 차원의 벡터를 학습하였으니, 이를 이용하여 해당 단어와 topically relavant 한 문서들을 검색할 수 있습니다.\n",
    "\n",
    "각 문서에 대해 most frequent terms 를 확인하기 위하여 get_bow 함수를 만듭니다. 이 역시 utils.py 에 넣어둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_docs_from_term(term, wv, dv, vocab_to_idx, topn=10):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    term : str\n",
    "        Query term\n",
    "    wv : numpy.ndarray or scipy.sparse.matrix\n",
    "        Word representation matrix. shape = (n_terms, dim)\n",
    "    dv : numpy.ndarray or scipy.sparse.matrix\n",
    "        Document representation matrix. shape = (n_docs, dim)\n",
    "    vocab_to_idx : {str:int}\n",
    "        Word to index\n",
    "    topn : int\n",
    "        Number of most similar documents\n",
    "    \"\"\"\n",
    "\n",
    "    # encode term as index\n",
    "    idx = vocab_to_idx.get(term, -1)\n",
    "    if idx < 0:\n",
    "        return []\n",
    "\n",
    "    # prepare query term vector\n",
    "    query_vec = wv[idx,:].reshape(1,-1)\n",
    "\n",
    "    # compute distance between query term vector and document vectors\n",
    "    dist = pairwise_distances(\n",
    "        dv,\n",
    "        query_vec,\n",
    "        metric='cosine'\n",
    "    ).reshape(-1)\n",
    "\n",
    "    # find similar document indices\n",
    "    similar_doc_idx = dist.argsort()[:topn]\n",
    "\n",
    "    # return\n",
    "    return similar_doc_idx\n",
    "\n",
    "def get_bow(doc_idx, bow, idx_to_vocab, topn=10):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    term : str\n",
    "        Query term\n",
    "    bow : scipy.sparse.matrix\n",
    "        Term frequency matrix. shape = (n_docs, n_terms)\n",
    "    idx_to_vocab : list of str\n",
    "        Index to word\n",
    "    topn : int\n",
    "        Number of most frequent terms\n",
    "    \"\"\"\n",
    "\n",
    "    # get term frequency submatrix\n",
    "    x_sub = bow[doc_idx,:]\n",
    "\n",
    "    # get term indices and their frequencies\n",
    "    terms = x_sub.nonzero()[1]\n",
    "    freqs = x_sub.data\n",
    "\n",
    "    # format : [(term, frequency), ... ]\n",
    "    bow = [(term, freq) for term, freq in zip(terms, freqs)]\n",
    "    \n",
    "    # sort by frequency in decreasing order\n",
    "    bow = sorted(bow, key=lambda x:-x[1])[:topn]\n",
    "\n",
    "    # decode term index to vocabulary\n",
    "    bow = [(idx_to_vocab[term], freq) for term, freq in bow]\n",
    "\n",
    "    # return\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'오바마'와 관련된 문서들입니다. 2016-10 에는 미국 대선이 이뤄지던 기간입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc#=9615 : [('오바마', 11), ('트럼프', 11), ('대통령', 10), ('미국', 5), ('초대', 4), ('토론', 4), ('대변', 3), ('지지', 3), ('케냐', 3), ('힐러리', 3)]\n",
      "doc#=9471 : [('오바마', 13), ('대통령', 12), ('트럼프', 11), ('미국', 6), ('초대', 4), ('대변', 3), ('지지', 3), ('케냐', 3), ('토론', 3), ('부인', 2)]\n",
      "doc#=14951 : [('트럼프', 10), ('대통령', 7), ('클린턴', 7), ('후보', 7), ('공화당', 4), ('결과', 3), ('꼭두각시', 3), ('대선', 3), ('비방', 3), ('기자', 2)]\n",
      "doc#=7256 : [('트럼프', 25), ('힐러리', 17), ('대통령', 7), ('토론', 7), ('푸틴', 6), ('미국', 5), ('여자', 5), ('끔찍', 4), ('러시아', 4), ('꼭두각시', 3)]\n",
      "doc#=11929 : [('대통령', 7), ('오바마', 7), ('트럼프', 7), ('선거', 6), ('주장', 6), ('조작', 5), ('대선', 4), ('이라고', 4), ('클린턴', 4), ('후보', 4)]\n",
      "doc#=11441 : [('공화당', 20), ('대통령', 7), ('로비', 6), ('선거', 6), ('후보', 6), ('것이다', 5), ('미국', 5), ('조직', 4), ('트럼프', 4), ('권력', 3)]\n",
      "doc#=14219 : [('트럼프', 6), ('오바마', 4), ('토론', 4), ('대통령', 3), ('초청', 3), ('힐러리', 3), ('3차', 2), ('공화당', 2), ('민주당', 2), ('불편', 2)]\n",
      "doc#=30018 : [('토론', 12), ('트럼프', 9), ('후보', 7), ('클린턴', 6), ('3차', 5), ('대선', 4), ('대통령', 4), ('미국', 4), ('오바마', 4), ('토론장', 4)]\n",
      "doc#=27797 : [('트럼프', 31), ('클린턴', 20), ('대통령', 12), ('미국', 9), ('토론', 9), ('꼭두각시', 6), ('여성', 6), ('후보', 6), ('19일', 5), ('뉴스1', 5)]\n",
      "doc#=15196 : [('트럼프', 8), ('대선', 5), ('클린턴', 5), ('미국', 4), ('토론', 4), ('승자', 3), ('응답자', 3), ('겨냥', 2), ('결과', 2), ('공격', 2)]\n"
     ]
    }
   ],
   "source": [
    "similar_docs = most_similar_docs_from_term('오바마', wv, y, vocab_to_idx)\n",
    "\n",
    "for doc_idx in similar_docs:\n",
    "    bow = get_bow(doc_idx, x, idx_to_vocab)\n",
    "    print('doc#={} : {}'.format(doc_idx, bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc#=16490 : [('아이오아이', 7), ('엠카운트다운', 4), ('무대', 3), ('너무너무너무', 2), ('선보', 2), ('스포츠조선', 2), ('완전체', 2), ('잠깐', 2), ('활동', 2), ('20일', 1)]\n",
      "doc#=9228 : [('아이오아이', 6), ('무대', 4), ('엠카운트다운', 4), ('너무너무너무', 2), ('발랄', 2), ('완전체', 2), ('이날', 2), ('20일', 1), ('갓세븐', 1), ('금지', 1)]\n",
      "doc#=9894 : [('아이오아이', 5), ('너무너무너무', 4), ('엠카운트다운', 3), ('출연', 3), ('걸그룹', 2), ('무대', 2), ('박진영', 2), ('발랄', 2), ('이날', 2), ('잠깐', 2)]\n",
      "doc#=16489 : [('선보', 5), ('아이오아이', 3), ('엠카운트다운', 3), ('무대', 2), ('스포츠조선', 2), ('이날', 2), ('20일', 1), ('갓세븐', 1), ('검은색', 1), ('금지', 1)]\n",
      "doc#=21249 : [('무대', 5), ('너무너무너무', 3), ('아이오아이', 3), ('컴백', 3), ('잠깐', 2), ('진영', 2), ('100', 1), ('20일', 1), ('가창력', 1), ('개개인', 1)]\n",
      "doc#=21674 : [('방탄소년단', 11), ('무대', 9), ('1위', 7), ('다비치', 6), ('방송', 4), ('엠카운트다운', 4), ('올랐다', 4), ('감사', 3), ('샤이니', 3), ('10위', 2)]\n",
      "doc#=8011 : [('무대', 5), ('신용재', 5), ('아이오아이', 5), ('엠카운트다운', 5), ('선보', 4), ('공개', 3), ('컴백', 3), ('너무너무너무', 2), ('모습', 2), ('방송', 2)]\n",
      "doc#=26397 : [('1위', 4), ('무대', 4), ('아이오아이', 4), ('눈물', 3), ('방송', 3), ('방탄소년단', 3), ('보컬', 3), ('신용재', 3), ('24', 2), ('기자', 2)]\n",
      "doc#=25181 : [('아이오아이', 4), ('너무너무너무', 3), ('매력', 3), ('무대', 3), ('잠깐', 2), ('중독성', 2), ('20일', 1), ('감성적', 1), ('개개인', 1), ('개성', 1)]\n",
      "doc#=26364 : [('무대', 5), ('24', 3), ('선보', 3), ('신용재', 3), ('아이오아이', 3), ('기자', 2), ('너무너무너무', 2), ('보컬', 2), ('엠카운트다운', 2), ('잠깐', 2)]\n"
     ]
    }
   ],
   "source": [
    "similar_docs = most_similar_docs_from_term('아이오아이', wv, y, vocab_to_idx)\n",
    "\n",
    "for doc_idx in similar_docs:\n",
    "    bow = get_bow(doc_idx, x, idx_to_vocab)\n",
    "    print('doc#={} : {}'.format(doc_idx, bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
